# task05 PyTorch模型定义的方式
---
（本学习笔记来源于[DataWhale-深入浅出PyTorch](https://github.com/datawhalechina/thorough_pytorch)）
本节学习：

- 熟悉PyTorch中模型定义的三种方式：Sequential，ModuleList和ModuleDict
- 读懂GitHub上千奇百怪的写法
- 自己根据需要灵活选取模型定义方式



## 5.1  `Module `知识回顾

- `Module`类是`torch.nn`模块里提供的一个模型构造类(`nn.Module`),是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型；
-  PyTorch模型定义应包括两个主要部分：各个部分的初始化(__init__)和数据流向定义(forward)


## 5.2 `nn.moudule`之Squential 

`nn.Sequential`用于**按顺序**包装一组网络层。


```python
import torch
import torchvision
import torch.nn as nn
from collections import OrderedDict


class LeNetSequential(nn.Module):
    def __init__(self, classes):
        super(LeNetSequential, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 6, 5),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(6, 16, 5),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2), )
        self.classifier = nn.Sequential(
            nn.Linear(16 * 5 * 5, 120),
            nn.ReLU(),
            nn.Linear(120, 84),
            nn.ReLU(),
            nn.Linear(84, classes), )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size()[0], -1)
        x = self.classifier(x)
        return x


class LeNetSequentialOrderDict(nn.Module):
    def __init__(self, classes):
        super(LeNetSequentialOrderDict, self).__init__()
        self.features = nn.Sequential(OrderedDict({
            'conv1': nn.Conv2d(3, 6, 5),
            'relu1': nn.ReLU(inplace=True),
            'pool1': nn.MaxPool2d(kernel_size=2, stride=2),
            'conv2': nn.Conv2d(6, 16, 5),
            'relu2': nn.ReLU(inplace=True),
            'pool2': nn.MaxPool2d(kernel_size=2, stride=2),
        }))

        self.classifier = nn.Sequential(OrderedDict({
        'fc1': nn.Linear(16 * 5 * 5, 120),
        'relu3': nn.ReLU(),
        'fc2': nn.Linear(120, 84),
        'relu4': nn.ReLU(inplace=True),
        'fc3': nn.Linear(84, classes),
    }))


    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size()[0], -1)
        x = self.classifier(x)
        return x


```


```python
net = LeNetSequential(classes=2)
net = LeNetSequentialOrderDict(classes=2)


```


```python
fake_img = torch.randn((4, 3, 32, 32), dtype=torch.float32)


```


```python
output = net(fake_img)

print(net)
print(output)
```

## 5.3 `nn.moudule`之ModuleList

`nn.ModuleList`用于包装一组网络层，以**迭代**方式调用网络层

主要方法：
- `append()`:在ModuleList后面添加网络层
- `extend()`:拼接两个ModuleList
- `insert()`:指定在ModuleList中位置**插入**网络层


```python
class ModuleList(nn.Module):
    def __init__(self):
        super(ModuleList, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(20)])

    def forward(self, x):
        for i, linear in enumerate(self.linears):
            x = linear(x)
        return x
```

## 5.4 `nn.moudule`之ModuleDict

`nn.ModuleDict`用于包装一组网络层，以**索引**方式调用网络层

主要方法：

- `clear()`:清空ModuleDict
- `items()`:返回可迭代的键值对(key-value pairs)
- `keys()`:返回字典的键(key)
- `values()`:返回字典的值（value）
- `pop()`:返回一对键值，并从字典中删除



```python
class ModuleDict(nn.Module):
    def __init__(self):
        super(ModuleDict, self).__init__()
        self.choices = nn.ModuleDict({
            'conv': nn.Conv2d(10, 10, 3),
            'pool': nn.MaxPool2d(3)
        })
        self.activations = nn.ModuleDict({
            'relu': nn.ReLU(),
            'prelu': nn.PReLU()
        })

    def forward(self, x, choice, act):
        x = self.choices[choice](x)
        x = self.activations[act](x)
        return x


net = ModuleDict()
fake_img = torch.randn((4, 10, 32, 32))
output = net(fake_img, 'conv', 'relu')
print(output)

```

    tensor([[[[0.0000, 0.0000, 0.6661,  ..., 0.0000, 0.4233, 0.0715],
              [0.0000, 0.0000, 0.1219,  ..., 0.6663, 0.0000, 0.0419],
              [0.0000, 0.0000, 0.2731,  ..., 0.5060, 0.0000, 0.1619],
              ...,
              [0.0000, 0.6161, 0.0000,  ..., 0.0000, 0.1367, 0.5291],
              [0.0000, 0.8069, 0.0000,  ..., 0.0000, 0.1007, 0.5369],
              [0.0143, 0.5083, 0.2260,  ..., 0.0000, 0.0000, 0.0000]],
    
             [[0.6459, 0.0000, 0.0000,  ..., 0.0000, 0.5090, 0.0000],
              [0.0548, 0.2861, 0.5799,  ..., 1.3627, 0.0000, 0.0000],
              [0.7538, 0.0000, 0.3612,  ..., 0.5384, 0.0000, 0.0000],
              ...,
              [0.0000, 0.0000, 0.2894,  ..., 0.0000, 0.0000, 0.2972],
              [0.0000, 0.3672, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 1.4488, 0.4965,  ..., 0.0000, 0.0000, 0.6022]],
    
             [[0.1347, 0.1291, 0.0000,  ..., 0.2189, 0.0000, 0.7473],
              [0.6860, 0.5748, 0.0000,  ..., 0.0000, 0.1640, 0.2840],
              [0.0000, 0.3390, 0.0000,  ..., 0.0000, 0.8967, 0.0000],
              ...,
              [0.0000, 0.7837, 0.0000,  ..., 0.5569, 0.0000, 0.0000],
              [1.0068, 0.5863, 0.9679,  ..., 0.0000, 0.1922, 1.2686],
              [0.5492, 0.0000, 0.3465,  ..., 0.3041, 0.0000, 0.3506]],
    
             ...,
    
             [[0.0000, 0.0000, 0.2966,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 0.3922, 0.6658,  ..., 0.3831, 0.0000, 0.1765],
              [0.0000, 0.1390, 0.4767,  ..., 0.0000, 0.0000, 0.0285],
              ...,
              [0.5064, 0.0000, 0.8289,  ..., 0.0000, 0.0000, 0.3397],
              [0.1819, 0.1758, 0.0000,  ..., 0.1795, 0.8237, 0.0000],
              [0.3371, 0.0000, 0.0000,  ..., 0.0000, 0.5845, 0.3574]],
    
             [[0.0000, 0.0000, 0.4901,  ..., 0.9090, 0.0696, 0.0000],
              [0.6232, 0.3055, 0.2318,  ..., 0.3205, 0.0000, 0.0000],
              [0.4570, 0.1003, 0.0000,  ..., 0.2156, 0.0000, 0.0000],
              ...,
              [0.1836, 1.1044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.8988,  ..., 0.0000, 0.0000, 0.2276],
              [0.0000, 0.0037, 0.1857,  ..., 0.0000, 0.0000, 0.0000]],
    
             [[0.8215, 0.0000, 0.1955,  ..., 0.3878, 0.1152, 0.0000],
              [0.5620, 1.6681, 0.0000,  ..., 0.2504, 0.2628, 0.0000],
              [0.3883, 0.0000, 0.3868,  ..., 0.0000, 0.9569, 0.4384],
              ...,
              [0.0000, 0.0000, 0.5991,  ..., 0.0000, 0.2441, 0.0000],
              [0.0000, 0.1866, 0.5506,  ..., 0.0864, 0.2636, 0.3574],
              [0.0000, 0.2163, 0.0000,  ..., 0.1092, 0.0000, 0.4121]]],
    
    
            [[[0.0000, 0.0000, 0.0226,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.1166,  ..., 0.0000, 0.0000, 0.5844],
              [0.0000, 2.1938, 0.7413,  ..., 0.4713, 0.6901, 0.0000],
              ...,
              [0.4677, 0.4382, 1.0477,  ..., 0.1077, 0.0000, 0.0000],
              [0.0000, 0.0384, 0.0045,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 0.1559, 0.0000,  ..., 0.0000, 0.5982, 0.0000]],
    
             [[0.0000, 0.2133, 0.6420,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 0.2493, 0.0000,  ..., 0.2597, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
              ...,
              [0.0000, 0.0000, 0.7479,  ..., 0.0000, 0.3195, 0.1234],
              [0.0000, 0.4656, 0.7238,  ..., 0.0000, 0.1999, 0.0766],
              [0.3265, 0.0000, 0.0000,  ..., 0.0000, 0.7115, 0.0701]],
    
             [[0.6029, 0.0865, 0.4482,  ..., 0.0000, 0.3963, 0.3620],
              [0.0000, 0.3142, 0.0000,  ..., 0.0000, 0.1089, 0.0905],
              [1.0408, 0.0000, 0.2384,  ..., 0.0000, 0.0000, 1.1239],
              ...,
              [0.8398, 0.1578, 0.2845,  ..., 0.6468, 0.2973, 0.6795],
              [0.5347, 0.0000, 0.0000,  ..., 0.6311, 0.8298, 0.8256],
              [0.8013, 0.3670, 0.9543,  ..., 0.0000, 0.6810, 0.0000]],
    
             ...,
    
             [[0.4159, 0.7879, 0.0000,  ..., 0.0000, 0.7012, 0.0000],
              [0.0000, 0.0000, 0.0000,  ..., 0.1133, 0.0000, 0.5574],
              [0.0958, 0.2360, 0.3159,  ..., 0.0000, 0.6808, 0.0000],
              ...,
              [0.1354, 0.0995, 0.0000,  ..., 0.3007, 0.0000, 0.6803],
              [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1032, 0.1865],
              [0.0000, 0.4601, 0.0000,  ..., 0.1996, 0.0000, 0.0000]],
    
             [[0.0000, 0.0000, 0.1412,  ..., 0.0000, 0.4427, 0.2815],
              [0.0000, 0.2948, 0.0000,  ..., 0.8620, 0.0000, 0.0000],
              [0.0000, 0.8831, 0.0000,  ..., 0.0000, 0.4728, 0.2098],
              ...,
              [0.0000, 0.1367, 0.1630,  ..., 0.0000, 0.0000, 0.2429],
              [0.0000, 0.3994, 0.8719,  ..., 0.1287, 0.0000, 0.0433],
              [0.5465, 0.0000, 0.2486,  ..., 0.9296, 0.9423, 0.2181]],
    
             [[0.0000, 0.0083, 1.3656,  ..., 0.7775, 1.2073, 0.2617],
              [0.0000, 0.9124, 0.0000,  ..., 0.0000, 0.6010, 0.0000],
              [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
              ...,
              [0.4183, 0.0000, 0.6563,  ..., 0.0000, 0.9110, 0.4627],
              [0.0000, 0.0000, 0.3556,  ..., 0.9930, 0.0000, 0.1692],
              [0.2789, 0.3164, 0.0000,  ..., 0.0000, 0.9742, 0.2957]]],
    
    
            [[[0.0000, 0.9609, 0.2965,  ..., 0.2580, 0.8178, 0.0000],
              [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1827, 0.5884],
              [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7158, 0.0000],
              ...,
              [0.5464, 0.6637, 0.0000,  ..., 0.0921, 0.3805, 0.5839],
              [0.0000, 0.8111, 0.1255,  ..., 0.0000, 0.0000, 1.1074],
              [0.0000, 0.1309, 0.0000,  ..., 0.0000, 0.3827, 0.0000]],
    
             [[0.0000, 0.4022, 0.0859,  ..., 0.0000, 0.0000, 0.0000],
              [0.3304, 0.0143, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 0.2778, 0.0000,  ..., 0.3164, 0.4835, 1.0717],
              ...,
              [0.0000, 0.0682, 0.6345,  ..., 0.0000, 0.7048, 0.1418],
              [0.0000, 0.3146, 0.0000,  ..., 0.6443, 0.0000, 0.6206],
              [0.4640, 0.1817, 0.0000,  ..., 0.0000, 0.0794, 0.1594]],
    
             [[0.0000, 0.0000, 0.0000,  ..., 0.1595, 0.3716, 0.9515],
              [0.2232, 0.8134, 0.4349,  ..., 0.2625, 0.3616, 0.0000],
              [1.0048, 0.0000, 0.8376,  ..., 0.0000, 0.0000, 0.0000],
              ...,
              [0.0778, 0.0671, 0.7004,  ..., 0.4570, 0.0000, 0.0000],
              [0.0000, 1.1337, 1.1919,  ..., 0.2015, 0.0000, 0.0000],
              [0.0000, 0.7251, 0.7977,  ..., 0.8087, 1.1235, 0.0000]],
    
             ...,
    
             [[0.5686, 0.0000, 0.0000,  ..., 0.4319, 0.5184, 0.0000],
              [0.0000, 0.0000, 0.0193,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6394, 0.0000],
              ...,
              [0.0461, 0.0000, 0.0000,  ..., 0.0000, 0.3753, 0.1057],
              [0.0174, 0.6659, 0.0000,  ..., 0.6845, 0.5673, 0.3180],
              [0.0000, 0.0000, 1.0218,  ..., 0.0000, 1.7350, 0.0000]],
    
             [[0.0000, 0.7141, 0.0909,  ..., 0.0000, 0.0000, 0.3537],
              [0.0000, 0.0000, 0.5997,  ..., 0.0600, 0.7027, 0.0000],
              [0.0000, 0.0000, 0.3813,  ..., 0.0000, 0.0000, 0.0000],
              ...,
              [0.0000, 0.3399, 0.0000,  ..., 0.0000, 0.0000, 0.1291],
              [0.9878, 0.1652, 0.0000,  ..., 0.0000, 0.4215, 0.7620],
              [0.3839, 0.5474, 0.1463,  ..., 0.6705, 0.0000, 0.3745]],
    
             [[0.0000, 0.0395, 0.5573,  ..., 0.0000, 0.6153, 0.6479],
              [0.4281, 0.0000, 0.0735,  ..., 0.0000, 0.0000, 0.2855],
              [0.0628, 0.4539, 0.0000,  ..., 0.5410, 0.0000, 0.6275],
              ...,
              [0.0210, 0.1020, 0.0000,  ..., 0.0000, 0.2601, 0.0000],
              [0.3993, 0.1650, 0.0048,  ..., 1.3748, 0.0000, 0.0738],
              [0.0000, 0.4575, 0.0000,  ..., 0.5823, 0.0000, 0.0000]]],
    
    
            [[[0.0000, 0.0000, 0.3737,  ..., 0.4335, 0.0000, 0.0074],
              [0.0000, 0.1435, 0.0000,  ..., 0.0000, 0.0000, 0.8003],
              [0.0000, 0.0921, 0.0000,  ..., 0.0000, 0.0000, 0.4103],
              ...,
              [0.0000, 0.3979, 0.0000,  ..., 0.0000, 0.0662, 0.0000],
              [0.0000, 0.0000, 0.1285,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 0.5150, 0.0000,  ..., 0.0000, 0.0000, 0.6608]],
    
             [[0.0000, 0.0000, 0.0000,  ..., 1.0403, 0.0000, 0.0000],
              [0.0000, 0.0263, 0.0317,  ..., 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.3151,  ..., 0.0000, 0.0000, 0.3741],
              ...,
              [0.0000, 0.4998, 0.0000,  ..., 0.5867, 0.0000, 0.4479],
              [0.0000, 0.3855, 0.0000,  ..., 0.0000, 0.0000, 0.1471],
              [0.0000, 0.1353, 0.0000,  ..., 0.2990, 0.0000, 0.2862]],
    
             [[0.0000, 1.2090, 0.9880,  ..., 0.0000, 0.0000, 0.8681],
              [0.4557, 0.6919, 0.0000,  ..., 0.4316, 0.6277, 0.2615],
              [0.7877, 0.0000, 0.0000,  ..., 0.5036, 0.9505, 0.9174],
              ...,
              [0.0000, 0.0000, 0.5322,  ..., 1.0588, 0.0000, 0.1386],
              [0.0000, 0.4012, 0.0470,  ..., 0.1988, 1.6754, 0.0000],
              [0.0000, 0.0000, 0.6148,  ..., 0.0000, 0.0000, 0.1263]],
    
             ...,
    
             [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2817],
              [0.0000, 0.9431, 0.6375,  ..., 0.0000, 0.0000, 0.0000],
              [0.0606, 0.4211, 0.0000,  ..., 0.0000, 0.3300, 0.0000],
              ...,
              [0.1719, 0.4770, 0.0000,  ..., 0.0230, 0.9551, 0.0000],
              [0.0000, 0.0000, 0.1566,  ..., 0.0000, 0.0000, 0.1450],
              [0.0000, 1.2392, 0.1134,  ..., 0.3290, 0.0000, 0.6556]],
    
             [[0.3918, 0.9993, 0.0000,  ..., 0.0000, 0.4521, 0.4753],
              [0.3562, 0.5186, 0.1300,  ..., 0.0000, 0.9157, 0.0000],
              [0.0000, 0.0038, 0.5747,  ..., 0.0000, 0.0000, 0.6737],
              ...,
              [0.1757, 0.6425, 0.0000,  ..., 0.2749, 0.1936, 0.5820],
              [0.0000, 0.8528, 0.0000,  ..., 0.2160, 0.0000, 0.0790],
              [0.0000, 0.0000, 0.3172,  ..., 0.4531, 0.0000, 0.2521]],
    
             [[0.2553, 0.6481, 0.0000,  ..., 0.0000, 0.0000, 0.8713],
              [0.6267, 0.0000, 0.4137,  ..., 1.6667, 0.0000, 0.0000],
              [0.2783, 0.0000, 0.3034,  ..., 0.0000, 0.0000, 0.0000],
              ...,
              [0.4292, 0.0000, 0.8014,  ..., 1.3004, 0.0000, 0.0000],
              [0.0000, 1.2741, 0.0000,  ..., 0.6711, 0.5453, 0.0095],
              [0.8290, 0.7556, 0.7288,  ..., 0.0000, 0.7804, 0.3357]]]],
           grad_fn=<ReluBackward0>)
    

## 5.5 三模块总结：

- `nn.Sequential`：**顺序性**，各网络之间严格按顺序执行，常用于block构建
- `nn.ModuleList`: **迭代性**，常用于大量重复网构建，通过for循环实现重复构建
- `nn.ModuleDict`: **索引性**，常用于可选择的网络层


```python

```
