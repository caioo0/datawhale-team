# Part A: 决策树（上）
----
>（本学习笔记来源于[DataWhale-树模型与集成学习](https://datawhalechina.github.io/machine-learning-toy-cod)）  
```md
“迭代者为人,递归者为神.” ---(L. Peter Deutsch)
```

`非线性模型`，`可分类`，`可回归`，`树形结构`，`以实例为基础的归纳学习`，`基本思想自顶向下`，`以信息增益（信息增益比，基尼系数等）为度量构建一颗度量标准下降最快的树`  


【内容概要】
- 掌握节点分类指标的引入原因、定义和计算
- 掌握两种分类树的原理和区别
- 掌握cart树与树减枝的原理


## 知识点
---

### 1. 信息论基础 

决策树思想： 对分类问题而言，决策树的思想是用节点代表样本集合，通过某些判定条件来对节点内的样本进行分配，将它们划分到该节点下的子节点，并且要求各个子节点中类别的纯度之和应高于该节点中的类别纯度，从而起到分类效果。

节点纯度：反映节点样本标签的不确定性；

- 纯度较低时，说明每种类别都倾向于以比较均匀的频率出现，从而我们较难在这个节点上得到关于样本标签的具体信息，其不确定性较高。
- 纯度很高时，说明有些类别倾向于以比较高的频率出现，从而我们能够更有信心地把握这个节点样本标签的具体信息，即确定性较高。

不确定性的度量（信息熵）：

$$
H(P_1,P_2,...,P_n) = - C\sum_{i=1}^{n}p_{i}logp_{i} = E(log\frac{1}{p_i})
$$

香浓给出信息熵满足三个条件：

- 连续型
- 等概时的单调增函数特性
- 可加性

上述三条件的数学表达式：

$$
 H(X) = H(P_{i}) ,其中 P_{i} 为分布函数
$$

> 熵是连续的
$$
g(N) = f(\frac{1}{N},...,\frac{1}{N}) 如果M \geq N , g(M)  \geq g(N)
$$

| 熵在等概率的时候，应该是单调函数 ，其物理意义就是信息量越大，编码长度越大
$$
H(p_1,...,p_{i-1},p_{i+1},...,p_n,p_{i1},p_{i2})=H(p_1,...,p_n)+p_iH(\frac{p_{i1}}{p_i}, \frac{p_{i2}}{p_i}) 

\ 其中 p_{i1}+p_{i2}=p_i
$$


信息熵：$H(Y)=-\sum_{k=1}^K p(y_k)\log_2p(y_k)$

条件熵: $H(Y|X) = \mathbb{E}_{X}[\mathbb{E}_{Y\vert X}[-\log_2p(Y\vert X)]]$


信息增益： $G(Y,X)=H(Y)-H(Y\vert X)$
## 2. 分类树的结点分裂

特征的类别可以分为三种情况讨论：`类别特征`、`数值特征`和`含缺失值的特征`，它们各自的处理方法略有不同。

**类别特征**

每个节点选择最大信息增益 $G_{N}^{max}$对应的特则进行分裂，直到所有节点的相对最大信息增益
$\frac{D_{N}}{D_{all}}G_{N}^{max}(Y,X) 小于 \epsilon$ ，这种生成算法称为ID3算法。在sklearn中，ϵ即为min_impurity_decrease。

C4.5算法在ID3算法的基础上做出了诸多改进，包括但不限于：处理数值特征、处理含缺失值的特征、使用信息增益比代替信息增益以及给出树的剪枝策略。

**数值特征**

- 1. 最佳分割法 对应了sklearn中splitter参数的best选项
- 2. 随机分割法 对应了sklearn中splitter参数的random选项


**含缺失值的特征**

C4.5算法处理缺失数据的思想非常简单，样本的缺失值占比越大，那么对信息增益的惩罚就越大，这是因为缺失值本身就是一种不确定性成分。

在sklearn中提供了两种生长模式，它们分别被称为深度优先生长和最佳增益生长，当参数max_leaf_nodes使用默认值None时使用前者，当它被赋予某个数值时使用后者。


深度优先生长采用深度优先搜索的方法：若当前节点存在未搜索过的子节点，则当前节点跳转到子节点进行分裂决策；若当前节点为叶节点，则调转到上一层节点，直到根节点不存在未搜索过的子节点为止。对上图而言，当前节点为2号，它的两个子节点4号和5号都没有被搜索过，因此下一步则选择两个节点中的一个进行跳转。当决策树使用最佳增益生长时，每次总是选择会带来最大相对信息增益的节点进行分裂，直到叶节点的最大数量达到max_left_nodes。





决策树的学习三个重要步骤：**特征选择**，**决策树的生成**以及**决策树的剪枝**。  
- **特征选择：** 常用的特征选择有信息增益，信息增益比，基尼系数等。
- **生成过程：** 通过计算信息增益或其他指标，选择最佳特征。从根结点开始，递归地产生决策树，不断的选取局部最优的特征，将训练集分割成能够基本正确分类的子集。
- **剪枝过程：** 首先定义决策树的评价指标，对于所有的叶子结点，累加计算每个叶子结点中的（样本数）与其（叶子节点熵值）的乘积，以叶子数目作为正则项（它的系数为剪枝系数）。然后计算每个结点的剪枝系数，它的大概含义是删除该结点的子树，损失不变的前提下，正则项系数的值为多少，这个值越小说明该子树越没有存在的必要。依次选取剪枝系数最小的结点剪枝，得到决策树序列，通过交叉验证得到最优子树。

## 知识回顾 
---
#### 1、ID3树算法、C4.5树算法和CART算法之间有何异同？ 

| 类型     | ID3                                                          | C4.5                                                     | **CART**                                                 |
| -------- | ------------------------------------------------------------ | -------------------------------------------------------- | -------------------------------------------------------- |
| 提出年份 | 1986                                                         | 1993                                                     | 1984                                                     |
| 解决问题 | 分类                                                         | 分类                                                     | 分类、回归                                               |
| 划分指标 | 信息增益                                                     | 信息增益比                                                | 基尼系数                                                 |
| 指标特点 | 偏向可取值数目较多的属性                                     | 偏向可取值数目较少的属性                                 |                                                          |
| 属性选择 | 选择信息增益最大的属性                                       | 先找出信息增益高于平均水平的属性，再从中选择增益率最高的 | 选择划分后基尼指数最小的属性                             |
| 划分规则 | 多叉                                                         | 多叉                                                     | 二叉，所以选择最优特征后还需要决定该特征的最优二值切分点 |
| 优缺点   | - DI3会偏向可取值数目加多的属性<br/>ID3算法并未给出处理连续数据的方法<br/>ID3算法不能处理带有缺失值的数据集<br/>ID3算法只有树的生成, 所以容易过拟合 | C4.5可以处理连续值<br/>C4.5时间耗费大                    | CART可以解决回归问题                                     |

#### 2、 什么是信息增益？它衡量了什么指标？它有什么缺陷？

- **信息增益:** 信息熵 - 条件熵 ，公式如：$G(D,A) = H(D) - H(D|A)$
- **信息增益:** 用于特征选择，评判特征的分类能力；
- **缺点:** 信息增益偏向取值较多的特征;

#### 3、 sklearn决策树中的random_state参数控制了哪些步骤的随机性？

  random_state是一个**随机种子**，是在**任意带有随机性的类或函数里作为参数来控制随机模式**。  
  当random_state取某一个值时，在以下步骤确保相同：  
  1. 训练集测试集的划分  
  2. 确保决策树模型相同  

#### 4、 决策树如何处理连续变量和缺失变量？

 1. 连续值问题: 对于连续变量一般离散化处理，将连续型变量的值划分到不同的区间，在C4.5决策树算法中，采用二分法处理连续型变量；  
 2. 缺失值处理：  
  - 缺失样本数量比较少，采用抛弃缺失值  
  - 补充缺失值  
  - 概率化缺失值  
  - 缺失值单独分支  

#### 5、基尼系数是什么？为什么要在CART中引入它？

  基尼系数（基尼不纯度）：表示在样本集合中一个随机选中的样本被分错的概率。 
  公式：$G = 1 - \sum_{i=1}^k p_i^2$  

  信息熵和基尼系数都是决策树中根节点划分的依据，本质上这两种方式没有太大的差别，具体的比较在这列一下：   
  - 信息熵的计算比基尼系数的稍慢一些，因为信息熵的公式里是求对数$log$，而基尼系数公式中采用平方求和。
  - Scikit Learn中的决策树默认使用基尼系数方式，所以当我们不传入`criterion`参数时，默认使用`gini`方式。
  - 信息熵和基尼系数没有特别的效果优劣。 

  在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。  
  在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。  
  但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。  
  能不能简化模型同时也不至于完全丢失熵模型的优点呢？  
  CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。  
  CART分类树算法建立起来的是二叉树，而不是多叉树；

#### 6、什么是树的预剪枝和后剪枝？具体分别是如何操作的？

剪枝是防止决策树过拟合的方法。一棵完全生长的决策树很可能失去泛化能力，因此需要剪枝。  
1）剪枝的策略  
剪枝分为预剪枝和后剪枝两种，预剪枝是在构建决策树时抑制它的生长，后剪枝是决策树生长完全后再对叶子节点进行修剪。  
2）预剪枝  
设置一个树的最大高度/深度或者为树设置一个最大节点数，达到这个值即停止生长  
对每个叶子节点的样本数设置最小值，生长时叶子节点样本数不能小于这个值  
判断每次生长对系统性能是否有增益  
方法 3 里，对于一个决策树，每次生长前，可以判断生长后系统在验证集上准确度是否提升，如果经过一次生长，系统在验证集上的准确度降低了，那么中止这次生长。  

3）后剪枝  
后剪枝方法是对一棵已经完全生长的决策树进行剪枝  
错误率降低剪枝（Reduced-Error Pruning）  
悲观剪枝（Pessimistic Error Pruning）  
代价复杂度剪枝（Cost-Complexity Pruning）  
我们重点介绍第一种。错误率降低剪枝的方法比较直观，从下至上遍历所有非叶子节点的子树，每次把子树剪枝（所有数据归到该节点，将数据中最多的类设为结果），
与之前的树在验证集上的准确率进行比较，如果有提高，则剪枝，否则不剪，直到所有非叶子节点被遍历完。

4）预剪枝和后剪枝的优缺点比较  
时间成本方面，预剪枝在训练过程中即进行剪枝，后剪枝要在决策树完全生长后自底向上逐一考察。显然，后剪枝训练时间更长。预剪枝更适合解决大规模问题。  
剪枝的效果上，预剪枝的常用方法本质上是基于贪心的思想，但贪心法却可能导致欠拟合，后剪枝的欠拟合风险很小，泛化性能更高。  
另外，预剪枝的有些方法使用了阈值，如何设置一个合理的阈值也是一项挑战。  


## 练习题 
---
【练习1】定义$X,Y$的联合熵为$H(Y,X)$为$E(Y,X)∼p(y,x)[−log2p(Y,X)]$  

请证明如下关系：  

$G(Y,X)=H(X)−H(X|Y)$    

$G(Y,X)=H(X)+H(Y)−H(Y,X)$    

$G(Y,X)=H(Y,X)−H(X|Y)−H(Y|X)$     


下图被分为了A、B和C三个区域。若AB区域代表X的不确定性，BC区域代表Y的不确定性，那么H(X)、H(Y)、H(X|Y)、H(Y|X)、H(Y,X)和G(Y,X)分别指代的是哪片区域？



【练习2】假设当前我们需要处理一个分类问题，请问对输入特征进行归一化会对树模型的类别输出产生影响吗？请解释原因。

概率模型不需要归一化，因为他们不关心变量的值，而是关心变量的分布和变量之间的条件概率。  
决策树是一种概率模型，数值缩放，不影响分裂点位置。所以一般不对其进行归一化处理。   

【练习3】如果将系数替换为1−γ2，请问对缺失值是加强了还是削弱了惩罚？

【练习】如果将树的生长策略从深度优先生长改为广度优先生长，假设其他参数保持不变的情况下，两个模型对应的结果输出可能不同吗？

【练习】在一般的机器学习问题中，我们总是通过一组参数来定义模型的损失函数，并且在训练集上以最小化该损失函数为目标进行优化。请问对于决策树而言，模型优化的目标是什么？

【练习】对信息熵中的log函数在p=1处进行一阶泰勒展开可以近似为基尼系数，那么如果在p=1处进行二阶泰勒展开我们可以获得什么近似指标？请写出对应指标的信息增益公式。

【练习】除了信息熵和基尼系数之外，我们还可以使用节点的1−maxkp(Y=yk)和第m个子节点的1−maxkp(Y=yk|X=xm)来作为衡量纯度的指标。请解释其合理性并给出相应的信息增益公式。

【练习】为什么对没有重复特征值的数据，决策树能够做到损失为0？

【练习】如何理解min_samples_leaf参数能够控制回归树输出值的平滑程度？



## 参考资料
- [1]. https://cloud.tencent.com/developer/article/1462716
- [2]. https://cloud.tencent.com/developer/article/1475554
- [3]. https://www.cnblogs.com/starfire86/p/5749334.html
- [4]. https://zhuanlan.zhihu.com/p/140376729



