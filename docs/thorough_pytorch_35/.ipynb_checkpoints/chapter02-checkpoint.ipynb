{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ecf87b",
   "metadata": {},
   "source": [
    "\n",
    "# 第二章 PyTorch基础知识\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 数据操作\n",
    "\n",
    "深度学习存储和操作数据的主要接口是张量（\\(n\\)维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。\n",
    "\n",
    "几何代数中定义的张量是基于向量和矩阵的推广，比如我们可以将标量视为零阶张量，矢量可以视为一阶张量，矩阵就是二阶张量。\n",
    "\n",
    "- 0D：**标量** 标量是一个数字\n",
    "- 1D：**向量**  1维张量称为“向量”。\n",
    "- 2D： 2维张量称为**矩阵**\n",
    "- 3D： 公用数据存储在张量 时间序列数据 股价 文本数据 彩色图片(**RGB**)\n",
    "- 4D:  批量3维张量，比如10,000张彩色图片的集合；\n",
    "\n",
    "一个图像可以用三个字段表示：\n",
    "\n",
    "```\n",
    "(width, height, channel) = 3D\n",
    "```\n",
    "4D张量：\n",
    "\n",
    "```\n",
    "(sample_size, width, height, channel) = 4D\n",
    "````\n",
    "\n",
    "在PyTorch中， `torch.Tensor` 是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现 `Tensor` 和NumPy的多维数组非常类似。然而，`Tensor` 提供GPU计算和自动求梯度等更多功能，这些使 `Tensor` 这一数据类型更加适合深度学习\n",
    "\n",
    "\n",
    "### 2.1.1. 创建`Tensor`\n",
    "\n",
    "首先，我们导入`torch`。请注意，虽然它被称为PyTorch，但是代码中使用`torch`而不是pytorch。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086bcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f14b3f",
   "metadata": {},
   "source": [
    "首先，我们可以使用 arange 创建一个行向量 x。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。张量中的每个值都称为张量的 元素（element）。例如，张量 x 中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa59bd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407926c2",
   "metadata": {},
   "source": [
    "可以通过张量的shape属性来访问张量（沿每个轴的长度）的形状 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f8be46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf56a44",
   "metadata": {},
   "source": [
    "如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。 因为这里在处理的是一个向量，所以它的shape与它的size相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91515595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31edf7db",
   "metadata": {},
   "source": [
    "要想改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数。 例如，可以把张量x从形状为（12,）的行向量转换为形状为（3,4）的矩阵。 这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。 要重点说明一下，虽然张量的形状发生了改变，但其元素值并没有变。 注意，通过改变张量的形状，张量的大小不会改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2e3eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fc7e0",
   "metadata": {},
   "source": [
    "我们不需要通过手动指定每个维度来改变形状。 也就是说，如果我们的目标形状是（高度,宽度）， 那么在知道宽度后，高度会被自动计算得出，不必我们自己做除法。 在上面的例子中，为了获得一个3行的矩阵，我们手动指定了它有3行和4列。 幸运的是，我们可以通过-1来调用此自动计算出维度的功能。 即我们可以用x.reshape(-1,4)或x.reshape(3,-1)来取代x.reshape(3,4)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3066ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = x.reshape(-1,4)\n",
    "X1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf19eda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = x.reshape(3,-1)\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92833dae",
   "metadata": {},
   "source": [
    "有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a7c3b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c0855",
   "metadata": {},
   "source": [
    "同样，我们可以创建一个形状为(2,3,4,6)的张量，其中所有元素都设置为1。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff1b5eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2,3,4,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eac888",
   "metadata": {},
   "source": [
    "有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab80188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9263,  0.3160, -0.7383,  0.6722],\n",
       "        [ 0.3053, -1.6233,  0.2146, -0.2511],\n",
       "        [-0.6606, -0.5726, -0.1318, -1.1029]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3628d56",
   "metadata": {},
   "source": [
    "我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2fef184",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ea01b",
   "metadata": {},
   "source": [
    "我们可以通过shape或者size()来获取Tensor的形状:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ea21b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2b15a",
   "metadata": {},
   "source": [
    "> 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c89579",
   "metadata": {},
   "source": [
    "还有很多函数可以创建`Tensor`，去翻翻官方API就知道了，下表给了一些常用的作参考。\n",
    "\n",
    "\n",
    "|函数\t|功能|\n",
    "| --- |---|\n",
    "| Tensor(*sizes) | 基础构造函数|\n",
    "|tensor(data,)\t|类似np.array的构造函数|\n",
    "|ones(*sizes)\t|全1Tensor|\n",
    "|zeros(*sizes)\t|全0Tensor|\n",
    "|eye(*sizes)\t|对角线为1，其他为0|\n",
    "|arange(s,e,step)|\t从s到e，步长为step|\n",
    "|linspace(s,e,steps)\t|从s到e，均匀切分成steps份|\n",
    "|rand/randn(*sizes)|\t均匀/标准分布\n",
    "|normal(mean,std)/uniform(from,to)\t|正态分布/均匀分布|\n",
    "|randperm(m)\t随机排列|\n",
    "\n",
    "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c092c3",
   "metadata": {},
   "source": [
    "### 2.1.2. 操作 \n",
    "\n",
    "\n",
    "本小节介绍`Tensor`的各种操作。\n",
    "\n",
    "\n",
    "在数学表示法中，我们将通过符号 $f: \\mathbb{R} \\rightarrow \\mathbb{R}$  来表示一元标量运算符（只接收一个输入）。 这意味着该函数从任何实数（ $\\mathbb{R}$ ）映射到另一个实数。 同样，我们通过符号 $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$  表示二元标量运算符，这意味着该函数接收两个输入，并产生一个输出。 给定同一形状的任意两个向量 $u$ 和 $v$ 和二元运算符 $f$ ， 我们可以得到向量 $c=F(u,v)$ 。 具体计算方法是 $c_i←f(u_i,v_i)$ ， 其中 $c_i 、 u_i 和 v_i$ 分别是向量 $c 、 u 和 v$ 中的元素。 在这里，我们通过将标量函数升级为按元素向量运算来生成向量值  $F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ 。\n",
    "\n",
    "对于任意具有相同形状的张量， 常见的标准算术运算符$（+、-、*、/和**）$都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。 在下面的例子中，我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6476f802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2,4,8])\n",
    "y = torch.tensor([2,2,2,2])\n",
    "\n",
    "x + y, x - y,x * y,x/y,x**y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477fccca",
   "metadata": {},
   "source": [
    "**算术操作**\n",
    "\n",
    "在PyTorch中，同一种操作可能有很多种形式，下面用加法作为例子。\n",
    "\n",
    "- **加法形式一**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "064213f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5797, 1.0652, 1.1602],\n",
      "        [1.4770, 1.5374, 1.9931],\n",
      "        [1.7169, 1.9392, 1.6505],\n",
      "        [1.9700, 1.4227, 1.5914],\n",
      "        [1.4348, 1.9572, 1.8105]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "\n",
    "y = torch.rand(5, 3)\n",
    "\n",
    "print(x + y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de70aa6",
   "metadata": {},
   "source": [
    "- **加法形式二**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2cda800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5797, 1.0652, 1.1602],\n",
      "        [1.4770, 1.5374, 1.9931],\n",
      "        [1.7169, 1.9392, 1.6505],\n",
      "        [1.9700, 1.4227, 1.5914],\n",
      "        [1.4348, 1.9572, 1.8105]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbebec7",
   "metadata": {},
   "source": [
    "还可指定输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ac853ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5797, 1.0652, 1.1602],\n",
      "        [1.4770, 1.5374, 1.9931],\n",
      "        [1.7169, 1.9392, 1.6505],\n",
      "        [1.9700, 1.4227, 1.5914],\n",
      "        [1.4348, 1.9572, 1.8105]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90301d4e",
   "metadata": {},
   "source": [
    "- **加法形式三、inplace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9504fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5797, 4.0652, 4.1602],\n",
      "        [4.4770, 4.5374, 4.9931],\n",
      "        [4.7169, 4.9392, 4.6505],\n",
      "        [4.9700, 4.4227, 4.5914],\n",
      "        [4.4348, 4.9572, 4.8105]])\n"
     ]
    }
   ],
   "source": [
    "# adds x to y \n",
    "\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f6412",
   "metadata": {},
   "source": [
    "> 注：PyTorch操作inplace版本都有后缀_, 例如x.copy_(y), x.t_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104bba4",
   "metadata": {},
   "source": [
    "“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e8894355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7183, 2.7183, 2.7183],\n",
       "        [2.7183, 2.7183, 2.7183],\n",
       "        [2.7183, 2.7183, 2.7183],\n",
       "        [2.7183, 2.7183, 2.7183],\n",
       "        [2.7183, 2.7183, 2.7183]], dtype=torch.float64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff08827",
   "metadata": {},
   "source": [
    "除了按元素计算外，我们还可以执行线性代数运算，包括向量点积和矩阵乘法。\n",
    "\n",
    "\n",
    "我们也可以把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。 下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。 我们可以看到，第一个输出张量的轴-0长度（\\(6\\)）是两个输入张量轴-0长度的总和（\\(3 + 3\\)）； 第二个输出张量的轴-1长度（\\(8\\)）是两个输入张量轴-1长度的总和（\\(4 + 4\\)）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2277ca99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12,dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1.0, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X,Y),dim=0),torch.cat((X,Y),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3a98a",
   "metadata": {},
   "source": [
    "有时，我们想通过逻辑运算符构建二元张量。 以X == Y为例： 对于每个位置，如果X和Y在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句X == Y在该位置处为真，否则该位置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "359eecf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X==Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b61f5d",
   "metadata": {},
   "source": [
    "对张量中的所有元素进行求和，会产生一个单元素张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23ca5808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5384f2",
   "metadata": {},
   "source": [
    "### 2.1.3. 广播机制\n",
    "\n",
    "在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 \n",
    "\n",
    "这种机制的工作方式如下：\n",
    "- 首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 \n",
    "- 其次，对生成的数组执行按元素操作。\n",
    "\n",
    "在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059ccd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3,1))\n",
    "b = torch.arange(2).reshape((1,2))\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5eebf",
   "metadata": {},
   "source": [
    "由于a和b分别是$3\\times1$和$1\\times2$矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的$3\\times2$矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7fc8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae5593",
   "metadata": {},
   "source": [
    "### 2.1.4. 索引和切片\n",
    "\n",
    "就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。\n",
    "\n",
    "如下所示，我们可以用[-1]选择最后一个元素，可以用[1:3]选择第二个和第三个元素："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "113cd694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496b3c1",
   "metadata": {},
   "source": [
    "除读取外，我们还可以通过指定索引来将元素写入矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a37755e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1,2]=9\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05dabb",
   "metadata": {},
   "source": [
    "如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，[0:2, :]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。 虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6248557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2,:]=12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b1b50",
   "metadata": {},
   "source": [
    "除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:\n",
    "\n",
    "|函数|\t功能|\n",
    "| --- | --- |\n",
    "|index_select(input, dim, index)|\t在指定维度dim上选取，比如选取某些行、某些列|\n",
    "|masked_select(input, mask)|\t例子如上，a[a>0]，使用ByteTensor进行选取\n",
    "|nonzero(input)\t非0元素的下标|\n",
    "|gather(input, dim, index)\t|根据index，在dim维度上选取数据，输出的size与index一样|\n",
    "\n",
    "\n",
    "这里不详细介绍，用到了再查官方文档。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfc885",
   "metadata": {},
   "source": [
    "### 2.1.5 改变形状\n",
    "\n",
    "用`view()`来改变`Tensor`的形状：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "37840c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1,5) # -1 所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(),y.shape,z.shape)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4c563",
   "metadata": {},
   "source": [
    "注意`view()`返回的新`Tensor`与源`Tensor`虽然可能有不同的`size`，但是是共享`data`的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c2e26c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], dtype=torch.float64)\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x +=1\n",
    "print(x)\n",
    "print(y) # 也加了1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab23dbb",
   "metadata": {},
   "source": [
    "所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个`reshape()`可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用`clone`创造一个副本然后再使用`view`。[参考此处](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a6fff159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_cp =  x.clone().view(15)\n",
    "x-=1\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a21150",
   "metadata": {},
   "source": [
    "> 使用`clone`还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源`Tensor`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e83e2",
   "metadata": {},
   "source": [
    "另外一个常用的函数就是`item()`, 它可以将一个标量`Tensor`转换成一个Python number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9ebcc710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2106])\n",
      "0.21058855950832367\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b62def",
   "metadata": {},
   "source": [
    "### 2.1.6. 节省内存\n",
    "\n",
    "运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。\n",
    "\n",
    "在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。 这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b05257f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y+X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cc82d",
   "metadata": {},
   "source": [
    "幸运的是，执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如Y[:] = <expression>。 为了说明这一点，我们首先创建一个新的矩阵Z，其形状与另一个Y相同， 使用zeros_like来分配一个全\\(0\\)的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61ee8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 1561725302656\n",
      "id(Z): 1561725302656\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83727c3",
   "metadata": {},
   "source": [
    "如果在后续计算中没有重复使用X， 我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6ffe7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d5c2b",
   "metadata": {},
   "source": [
    "### 2.1.7. 转换为其他Python对象\n",
    "\n",
    "将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fd8a8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " torch.Tensor,\n",
       " array([[26., 25., 28., 27.],\n",
       "        [25., 26., 27., 28.],\n",
       "        [20., 21., 22., 23.]], dtype=float32),\n",
       " tensor([[26., 25., 28., 27.],\n",
       "         [25., 26., 27., 28.],\n",
       "         [20., 21., 22., 23.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A),type(B),A,B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45e9b3",
   "metadata": {},
   "source": [
    "要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fad64e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a,a.item(),float(a),int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e8ad5",
   "metadata": {},
   "source": [
    "其他一些尝试，关于`backward`,`grad`,`TensorDataset`,`DataLoader`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6975e21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.7500)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =torch.tensor(3.5,requires_grad=True)\n",
    "y =(x-1)*(x-2)*(x-3)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2702b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]), tensor([44, 55]))\n",
      "================================================================================\n",
      "tensor([1, 2, 3]) tensor(44)\n",
      "tensor([4, 5, 6]) tensor(55)\n",
      "tensor([7, 8, 9]) tensor(66)\n",
      "tensor([1, 2, 3]) tensor(44)\n",
      "tensor([4, 5, 6]) tensor(55)\n",
      "tensor([7, 8, 9]) tensor(66)\n",
      "tensor([1, 2, 3]) tensor(44)\n",
      "tensor([4, 5, 6]) tensor(55)\n",
      "tensor([7, 8, 9]) tensor(66)\n",
      "tensor([1, 2, 3]) tensor(44)\n",
      "tensor([4, 5, 6]) tensor(55)\n",
      "tensor([7, 8, 9]) tensor(66)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "b = torch.tensor([44, 55, 66, 44, 55, 66, 44, 55, 66, 44, 55, 66])\n",
    "train_ids = TensorDataset(a, b) \n",
    "# 切片输出\n",
    "print(train_ids[0:2])\n",
    "print('=' * 80)\n",
    "# 循环取数据\n",
    "for x_train, y_label in train_ids:\n",
    "    print(x_train, y_label)\n",
    "# DataLoader进行数据封装\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58d115b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steop:0, batch_x:tensor([1., 2., 3., 4., 5.]), batch_y:tensor([10.,  9.,  8.,  7.,  6.])\n",
      "steop:1, batch_x:tensor([ 6.,  7.,  8.,  9., 10.]), batch_y:tensor([5., 4., 3., 2., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "BATCH_SIZE = 5\n",
    "# linspace, 生成1到10的10个数构成的等差数列\n",
    "x = torch.linspace(1, 10, 10)\n",
    "y = torch.linspace(10, 1, 10)\n",
    "\n",
    "# 把数据放在数据库中\n",
    "torch_dataset = Data.TensorDataset(x, y)\n",
    "# 从数据库中每次抽出batch size个样本\n",
    "loader = Data.DataLoader(dataset=torch_dataset,\n",
    "                         batch_size=BATCH_SIZE, # x, y 是相差为1个数为10的等差数列, batch= 5, 遍历loader就只有两个数据\n",
    "                         shuffle=False, # 不打乱顺序,便于查看\n",
    "                         num_workers=0)\n",
    "\n",
    "def show_batch():\n",
    "    #for epoch in range(10):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        # training\n",
    "        print(\"steop:{}, batch_x:{}, batch_y:{}\".format(step, batch_x, batch_y)) #方便输出\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f3389",
   "metadata": {},
   "source": [
    "### 2.1.8 线性代数\n",
    "\n",
    "另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示\n",
    "\n",
    "\n",
    "|函数|\t功能|\n",
    "|----|----|\n",
    "|trace|\t对角线元素之和(矩阵的迹)|\n",
    "|diag|\t对角线元素|\n",
    "|triu/tril|\t矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm|\t矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/baddbmm..|\t矩阵运算|\n",
    "|t|\t转置|\n",
    "|dot/cross|\t内积/外积|\n",
    "|inverse|\t求逆矩阵|\n",
    "|svd\t|奇异值分解|\n",
    "\n",
    "PyTorch中的`Tensor`支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考[官方文档](https://pytorch.org/docs/stable/tensors.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c08adf",
   "metadata": {},
   "source": [
    "###  2.1.9 `Tensor` on GPU\n",
    "\n",
    "用方法`to()`可以将`Tensor`在**CPU和GPU**（需要硬件支持）之间相互移动。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8106d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码只有PyTorch GPU版本上才会执行\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x,device=device)   # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y \n",
    "    print(z)\n",
    "    print(z.to(\"cpu\",torch.double))        # to()还可以同时更改数据类型\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ffb6b",
   "metadata": {},
   "source": [
    "### 2.1.10. 练习\n",
    "\n",
    "**问题1.** 运行本节中的代码。将本节中的条件语句X == Y更改为X < Y或X > Y，然后看看你可以得到什么样的张量。\n",
    "\n",
    "**解答：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b1320bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[3, 4, 6, 5],\n",
       "         [6, 1, 2, 7],\n",
       "         [3, 5, 4, 9]]),\n",
       " tensor([[ True,  True,  True,  True],\n",
       "         [ True, False, False, False],\n",
       "         [False, False, False, False]]),\n",
       " tensor([[False, False, False, False],\n",
       "         [False,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12,dtype=torch.float32).reshape(3,4)\n",
    "Y = torch.tensor([[3,4,6,5],[6,1,2,7],[3,5,4,9]])\n",
    "X,Y,X<Y,X>Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee23c4",
   "metadata": {},
   "source": [
    "**问题2.**用其他形状（例如三维张量）替换广播机制中按元素操作的两个张量。结果是否与预期相同？\n",
    "\n",
    "**解答：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09d078ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 1]), torch.Size([3, 1, 3]), torch.Size([3, 4, 3]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12,dtype=torch.float32).reshape((3,4,1))\n",
    "Y = torch.arange(9,dtype=torch.float32).reshape((3,1,3))\n",
    "X.shape,Y.shape,(X+Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d0c7c",
   "metadata": {},
   "source": [
    "可以看到结果与预期相同，我们换一种来看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e1e933d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 1]), torch.Size([3, 3, 1]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12,dtype=torch.float32).reshape((3,4,1))\n",
    "Y = torch.arange(9,dtype=torch.float32).reshape((3,3,1))\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35eb8977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = (X+Y)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b47d9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 2]), torch.Size([3, 2, 1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12,dtype=torch.float32).reshape((3,2,2))\n",
    "Y = torch.arange(6,dtype=torch.float32).reshape((3,2,1))\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b80f38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X+Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696522f",
   "metadata": {},
   "source": [
    "符合逾期"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89442126",
   "metadata": {},
   "source": [
    "**结论：** 多维数据对于最后显示的二维数[x,y]，满足两个x={y_row=n时，(n|1)},y={x_column=m时，m|1}，广播才符合逾期；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4fa2a3",
   "metadata": {},
   "source": [
    "## 2.2 自动求梯度\n",
    "\n",
    "\n",
    "在深度学习中，我们经常需要对函数求梯度（gradient）。PyTorch提供的`autograd`包能够根据输入和前向传播过程自动构建计算图，并执行反向传播。\n",
    "\n",
    "### 2.2.1 概念\n",
    "\n",
    "`torch.Tensor `是这个包的核心类。如果设置它的属性` .requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后,可以调用` .backward()`，来自动计算所有的梯度。此`Tensor`的梯度将会累计到`.grad`属性。\n",
    "\n",
    "\n",
    "> 注意：在 `y.backward()` 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。\n",
    "\n",
    "如果不想一个张量被跟踪历史，可以调用` .detach() `将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用`with torch.no_grad()`将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（`requires_grad=True`）的梯度。\n",
    "\n",
    "`Function`是另外一个很重要的类。`Tensor `和` Function` 互相结合就可以构建一个记录有整个计算过程的有向无环图 (acyclic graph DAG)，每个张量`Tensor`都有一个` .grad_fn `属性，该属性创建该 `Tensor `的`Function`，就是说该`Tensor`是不是通过某些运算得到的，若是，则`grad_fn`返回一个与这些运算相关的对象，否则是None。\n",
    "\n",
    "下面通过一些例子来理解这些概念。\n",
    "\n",
    "\n",
    "### 2.2.1. `Tensor`举例\n",
    "\n",
    "作为一个演示例子，假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导。 首先，我们创建变量x并为其分配一个初始值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f379ef09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4bc349ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0405f",
   "metadata": {},
   "source": [
    "[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，我们需要一个地方来存储梯度。**]\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
    "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
    "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "55891848",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True) # 等价于 `x = torch.arange(4.0, requires_grad=True)`\n",
    "x.grad # 默认值是None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b423795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ff73e",
   "metadata": {},
   "source": [
    "现在我们计算$y$,根据函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f5837ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x0000016BA286B310>\n"
     ]
    }
   ],
   "source": [
    "y = 2 * torch.dot(x,x)\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afa121",
   "metadata": {},
   "source": [
    "注意$x$是直接创建的，所以它没有`grad_fn`, 而$y$是通过一个函数操作创建的，所以它有一个为`<MulBackward0>`的`grad_fn`。\n",
    "\n",
    "\n",
    "像$x$这种直接创建的称为叶子节点，叶子节点对应的`grad_fn`是`None`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5709bf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf, y.is_leaf) # True False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3008db",
   "metadata": {},
   "source": [
    "再来点复杂度运算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b72bf017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4., 5.], grad_fn=<AddBackward0>) tensor([12., 27., 48., 75.], grad_fn=<MulBackward0>) tensor(40.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y1 = x + 2\n",
    "z = y1 * y1 * 3\n",
    "out = z.mean()\n",
    "print(y1,z,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed28743",
   "metadata": {},
   "source": [
    "通过`.requires_grad_()`来用in-place的方式改变`requires_grad`属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a57320ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x0000016BA294C370>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.grad_fn)\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.grad_fn)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76940479",
   "metadata": {},
   "source": [
    "### 2.2.2 梯度\n",
    "\n",
    "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。接下来，我们[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。\n",
    "\n",
    "因为`y`是一个标量，所以调用`backward()`时不需要指定求导变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "693e2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # 等价于 y.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8554caa",
   "metadata": {},
   "source": [
    "我们来看看`y`关于`x`的梯度$\\frac{d_{(y)}}{d_x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "046db3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f76b6",
   "metadata": {},
   "source": [
    "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
    "让我们快速验证这个梯度是否计算正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d248c323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9101cb",
   "metadata": {},
   "source": [
    "**现在让我们计算`x`的另一个函数。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "636ebec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch会累计梯度，我们需要清除之前的值\n",
    "\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbd98c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a61fae8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5816eb",
   "metadata": {},
   "source": [
    "**非标量变量的反向传播**\n",
    "\n",
    "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
    "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
    "\n",
    "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），\n",
    "但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
    "这里(**，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61026162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "print(x)\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57e544f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb89cdf",
   "metadata": {},
   "source": [
    "## 2.3 并行计算\n",
    "\n",
    "利用PyTorch做深度学习的过程中，可能会遇到数据量较大无法在单块GPU上完成，或者需要提升计算速度的场景，这时就需要用到并行计算。本节让我们来简单地了解一下并行计算的基本概念和主要实现方式。\n",
    "\n",
    "### 2.3.1 为什么要做并行计算\n",
    "\n",
    "我们学习PyTorch的目的就是可以编写我们自己的框架，来完成特定的任务。可以说，在深度学习时代，GPU的出现让我们可以训练的更快，更好。所以，如何充分利用GPU的性能来提高我们模型学习的效果，这一技能是我们必须要学习的。\n",
    "\n",
    "### 2.3.2 `CUDA`\n",
    "\n",
    "`CUDA`是我们使用GPU的提供商——NVIDIA提供的GPU并行计算框架。对于GPU本身的编程，使用的是`CUDA`语言来实现的。但是，在我们使用PyTorch编写深度学习代码时，使用的`CUDA`又是另一个意思。在PyTorch使用 `CUDA`表示要开始要求我们的模型或者数据开始使用GPU了。\n",
    "\n",
    "在编写程序中，当我们使用了 `cuda()` 时，其功能是让我们的模型或者数据迁移到GPU当中，通过GPU开始计算。\n",
    "\n",
    "### 2.3.3 并行计算方法\n",
    "\n",
    "**1) 网络结构分布到不同的设备中(Network partitioning)**\n",
    "\n",
    "**逻辑思路：** 将一个模型的各个部分拆分，然后将不同的部分放入到GPU来做不同任务的计算。\n",
    "\n",
    "**缺点：** 不同模型组件在不同的GPU上时，GPU之间的传输就很重要，对于GPU之间的通信是一个考验。但是GPU的通信在这种密集任务中很难办到。所有这个方式慢慢淡出了视野。\n",
    "\n",
    "架构如图：\n",
    "\n",
    "![img.png](./images/chapter02-01.png)\n",
    "\n",
    "\n",
    "**2）同一层的任务分布到不同数据中(Layer-wise partitionign)**\n",
    "\n",
    "**逻辑思路：** 同一层的模型做一个拆分，让不同的GPU去训练同一层模型的部分任务。\n",
    "\n",
    "**缺点：** 这样可以保证在不同组件之间传输的问题，但是在我们需要大量的训练，同步任务加重的情况下，会出现和第一种方式一样的问题。\n",
    "\n",
    "其架构如下：\n",
    "\n",
    "![img.png](./images/chapter02-02.png)\n",
    "\n",
    "\n",
    "**3）不同的数据分布到不同的设备中，执行相同的任务(Data parallelism) **\n",
    "\n",
    "**逻辑思路：** 不再拆分模块，训练的时候模型都是一整个模型，但是将输入数据进行拆分。所谓的拆分数据是指，同一个模型在不同GPU中训练一部分数据，然后再分别计算一部分数据之后，只需要将输出的数据做一个汇总，然后再反传。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "其架构如下：\n",
    "\n",
    "![img.png](./images/chapter02-03.png)\n",
    "\n",
    "\n",
    "**现在的主流方式，这种方式可以解决之前模式遇到的通讯问题。**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b09c2",
   "metadata": {},
   "source": [
    "## 2.4 参考资料\n",
    "\n",
    "1. https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.3_autograd\n",
    "2. https://foxsen.github.io/archbase/%E5%A4%9A%E6%A0%B8%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84.html#nvidia-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c33d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b884e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch39",
   "language": "python",
   "name": "Pytorch39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
