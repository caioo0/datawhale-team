## task04:DQN算法基本原理

---

> （本学习笔记来源于DataWhale-第39期组队学习：[强化学习](https://linklearner.com/datawhale-homepage/#/learn/detail/91)） ,[B站视频讲解](https://www.bilibili.com/video/BV1HZ4y1v7eX) 观看地址

### 什么是DQN？

DQN是早期最经典的深度强化学习算法，作为Q-Learning算法的拓展（Q-Learning的坑还没填。。），其核心思想是利用神经网络代替表格式方法来完成 值函数近似 ，此外DQN算法还引入 目标网络（Target Network） 和 经验回放（Experience Replay） 的概念，提升了训练的稳定性和数据利用效率。

### 什么是值函数近似？

在Q-Learning算法中用于评估动作价值的函数被称为Q函数（状态-动作价值函数），它代表了在当前状态s下选择动作a后依据某个确定的策略π得到的预期累计奖励。我们优化的目标就是寻找合适的策略π，使得执行该策略得到的任意状态下的Q值最大化。对于Q-Learning中离散状态空间的值函数，我们可以用Q表来记录，但如果状态空间维度过高或在连续空间中，想要准确求解每个状态每个动作下的Q值就比较困难，通常需要对Q函数进行参数化的近似，在DRL中我们通常选择利用神经网络来拟合这个Q函数，这个过程就被称为值函数近似。

### 什么是目标网络？

在时间差分方法更新Q网络的过程中，根据优化

$$
Qπ(st,at)=rt+Qπ(st+1,π(st+1))Qπ(st,at)=rt+Qπ(st+1,π(st+1))

$$

，若左右两部分均发生变化则会使得训练过程不稳定，收敛困难。因此DQN中引入一个目标网络来与正在被更新的网络分开，目标网络基本保持稳定，用于计算目标Q值（即公式中右半部分的预测Q值），然后每隔一段时间才将当前Q网络的参数更新过来。

### 什么是经验回放？

因为强化学习的采样是一个连续的过程，前后数据关联性很大，而神经网络的训练通常要求数据样本是静态的独立同分布。因此我们在这里引入一个经验池（Replay Buffer）的概念，即将强化学习直接利用当前Q网络的策略采样得到的数据以元组的形式

$$
<st,at,rt,st+1><st,at,rt,st+1>

$$

存储下来，在训练时，从经验池中随机采样一些样本作为一个batch，然后用这个打乱后的batch对Q网络进行更新，而当Buffer装满后最早的数据样本就会被删除。

### 动态规划、蒙特卡洛与时序差分方法的异同

广义的动态规划（Dynamic Programming）是指将复杂问题分解为许多较小的问题来求解，而这些小问题的解可以集合起来反推最初复杂问题的解。在这个过程中需要我们对整个环境（模型）有完整的认识，而且需要对过程中已求解部分问题的结果进行一个储存。在MDP中我们利用值函数储存子问题的解，而用贝尔曼方程进行当前状态解与下一状态解之间的递归。注意，由于模型已知，我们只需要状态值函数V就可以确定策略，而在蒙特卡洛中则需要状态-动作值函数Q来估计每个动作的价值。

广义的蒙特卡洛方法（Monte Carlo Methods）是一种通过大量随机采样来近似构建系统模型的数值模拟方法。在处理MDP时，我们可以通过对序列的完整采样获得一个episode，如果有足够多的episode，我们甚至可以遍历MDP中可能出现的所有序列，即使做不到这一点，大量的采样也可以近似地体现MDP的特性。与动态规划方法不同的是，蒙特卡洛方法无需对模型有充分了解（如了解状态转移概率或即时奖励函数），而是相当于通过大量实验来进行经验总结，同时它对于值函数的估计也不是计算累积回报的期望，而是在充分采样的基础上，直接计算经验平均回报。通俗的说，就是在每个采样序列中记录任意状态s下的某动作a第一次出现后（因为一个状态-动作对可能在序列中出现多次）得到的后续累积回报，记作first-visit，通过多次采样，对(s,a)的first-visit累积回报求平均。由大数定理可知，当采样足够多，这个平均值就会接近价值函数在(s,a)处的取值Qπ(s,a)Qπ(s,a)

参考资料：

1. DataWhale组队学习资料——《强化学习》 王琦 杨毅远 江季 著
2. [https://www.cnblogs.com/yijuncheng/p/10138604.html](https://www.cnblogs.com/yijuncheng/p/10138604.html) 值函数近似——Deep Q-learning
3. [https://www.jianshu.com/p/1835317e5886](https://www.jianshu.com/p/1835317e5886) Reinforcement Learning笔记(2)--动态规划与蒙特卡洛方法
4. [https://zhuanlan.zhihu.com/p/114482584](https://zhuanlan.zhihu.com/p/114482584) 强化学习基础 Ⅱ: 动态规划，蒙特卡洛，时序差分
