# task03:策略梯度与PPO算法
> （本学习笔记来源于DataWhale-第39期组队学习：[强化学习](https://linklearner.com/datawhale-homepage/#/learn/detail/91)） ,
> [B站视频讲解](https://www.bilibili.com/video/BV1HZ4y1v7eX) 观看地址

## 策略梯度相关概念
什么是策略梯度方法？
策略梯度方法是相对于动作价值函数的另一类强化学习思路。在基于动作价值函数的方法中，我们需要先学习价值函数Q(s,a)，再根据估计的价值函数选择动作，价值函数相当于在不同状态下对不同动作的评分，是不可缺少的。而在策略梯度方法中，我们会直接学习动作策略，也就是说输出应当是当前状态下应该执行的动作，即π(a|s)=P(a|s)，实际上这里的动作是一个概率分布，更有利的动作会分配到更大的选择概率。因此策略梯度方法可以用包括神经网络在内的任意模型进行参数化，代表策略的参数向量我们用θ∈Rd′来表示，则t时刻下当状态为s、策略参数为θ时选择执行动作a的概率可写作：π(a|s,θ)=Pr{At=a|St=s,θt=θ}。

在所有的策略梯度类方法中，我们都会预先确定一个用于评价策略的某种性能指标，这里用J(θ)来表示。我们的目的是最大化这个性能指标，因此利用梯度上升对策略参数θ进行更新：

θt+1=θt+α∇J(θt)ˆ
这里的∇J(θt)ˆ∈Rd′实际上是一个随机估计，它的期望是选定的性能指标J对策略的参数θt的梯度∇J(θt)的近似。对参数更新也就是策略更新的方法，更新后的策略则直接指导动作的执行。在有些算法中，我们会同时学习策略和近似的价值函数，这类方法被称为actor-critic。

策略梯度方法与价值函数方法的比较
基于价值函数的方法很多，以经典的DQN为例，它以神经网络代替Q表来逼近最优Q函数，更新后的网络同时作为价值函数引导动作的选择，一定程度上解决了高维离散输入的问题，使得图像等信息的处理在强化学习中变得可能。但其仍存在一些问题，如：

无法表示随机策略，对于某些特殊环境而言，最优的动作策略可能是一个带有随机性的策略，因此需要按特定概率输出动作。
无法输出连续的动作值，比如连续范围内的温度数值。
价值函数在更新过程中的微小变动可能使得被选中的最优动作完全改变，在收敛过程中缺少鲁棒性。
相对而言，策略梯度算法可以较好地解决上述问题，而且策略的参数化允许我们通过参数模型的引入来添加先验知识。当然在有些情况下动作价值函数方法会更简单，更容易近似，有些情况下则相反，还是要根据实际情况选择采用的方法。


## 参考资料：

- [理解策略梯度算法](https://zhuanlan.zhihu.com/p/93629846)
- [强化学习进阶 第六讲 策略梯度方法] (https://zhuanlan.zhihu.com/p/26174099)
