# Task01: 绪论与深度学习概述、数学基础
----
>（本学习笔记来源于DataWhale-11月组队学习：[水很深的深度学习](https://datawhalechina.github.io/unusual-deep-learning/#/2.%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80)） ,[B站视频讲解](https://www.bilibili.com/video/BV1iq4y197L4?from=search&seid=15514429050345514467&spm_id_from=333.337.0.0) 观看地址
```md
“生活中有一些决定只有你可以选择。” ——(默尔·沙困)
```

## 绪论与深度学习概述

## 绪论

- **人工智能：** 是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。

**人工智能分为三类：**

1. 强人工智能：认为有可能制造出真正能推理和解决问题的智能机器，这样的机器被认为是有自主意识的
2. 弱人工智能：认为不可能制造出能真正进行推理和解决问题的智能 机器，这些机器只不过看起来像是智能的，但是并不真正拥有智能， 也不会有自主意识
3. 超级人工智能：机器的智能彻底超过了人类，“奇点”2050年到来？
   
- **人工智能的三次浪潮**

1. 第一次浪潮（1956-1976年，20年），最核心的是逻辑主义
   逻辑主义主要是用机器证明的办法去证明和推理一些知识，比如用机器证明一个数学定理。要想证明这些问题，需要把原来的条件和定义从形式化变成逻辑表达，然后用逻辑的方法去证明最后的结论是对的还是错的，也叫做逻辑证明。
2. 第二次浪潮（1976-2006年，30年），联结主义盛行
3. 第三次浪潮（2006-现在），基于互联网大数据的深度学习的突破

- **机器学习分类**
  
1. 有监督学习(SupervisedLearning)：有老师(环境)的情况下， 学生(计算机)从老师(环境)那里获得对错指示、最终答案的学习方法。`跟学师评`

2. 无监督学习(UnsupervisedLearning)：没有老师(环境)的情况下，学生(计算机)自学的过程，一般使用一些既定标准进行评价。`自学标评`

3. 强化学习(ReinforcementLearning)：没有老师(环境)的情况 下，学生(计算机)对问题答案进行自我评价的方法。`自学自评`

## 起源与发展

**第一阶段(1943 -1969)**

- 1943年：Warren McCulloch和Walter Pitts提出了MP神经元模型
- 1958年：Frank Rosenblatt提出了感知器(Perceptron)
- 1960年：Bernard Widrow和Ted Hoff提出了ADLINE神经网络
- 1969年：Marvin Minsky和Seymour Papert指出感知器只能做简单的线性分类任务，无法解决XOR这种简单分类问题

**第二阶段(1980 -1989)**

- 1982年：John Hopfield提出了Hopfield神经网络
- 1986年：David Rumelhart、Geoffrey Hinton和Ronald Williams提 出了误差反向传播算法(Error Back Propagation, BP)
- 1989年:YannLeCun等人提出了卷积神经网络(Convolutional Neural Networks，CNN)

**第三阶段(2006 - )**

- 2006年:Hinton和他的学生正式提出了深度学习的概念，通过无监督学习方法逐层训练算法，再使用有监督的反向传播算法进行调优

- 2011年:Frank Seide在语音识别基准测试数据集上获得压倒性优势

- 2012年:AlexKrizhevsky在CNN中引入ReLU激活函数，在图像识别基准测试中获得压倒性优势。

- 2012年：吴恩达(Andrew Ng)教授和谷歌首席架构师Jeff Dean共 同主导著名的GoogleBrain项目，采用16万个CPU来构建一个深层 神经网络——DNN，将其应用于图像和语音的识别，大获成功

2014年：Facebook的DeepFace项目，在人脸识别方面的准确率已 经能够达到97%以上，跟人类识别的准确率几乎没有差别

2016年：谷歌DeepMind开发的AlphaGo以4:1的比分战胜国际顶尖 围棋高手李世石，证明在围棋领域，基于深度学习技术的机器人已经超越了人类

## 深度学习的定义和主要应用

**定义**

- **深度学习定义：**一般是指通过训练多层网络结构对未知数据进行分类或回归

- **深度学习分类：**有监督学习方法——深度前馈网络、卷积神经网络、循环神经网络等；无监督学习方法——深度信念网、深度玻尔兹曼机，深度自编码器等。


**主要应用**
- 图像处理领域：物体识别
- 语音识别领域： 语音识别为文字，文字合成语音
- 自然语言处理领域：情感分析，翻译
- 其他综合应用
  

## 数学基础

## 1.微积分

#### 1.1 导数

**1.1.1导数定义**

**单变量函数的导数:**

$$
    f'(x)=\lim_{h\to{0}}\frac{f(x+h) - f(x)}{h}
$$

其几何解释为函数在点$x$处切线的斜率，物理解释为函数在点$x$处的变化率。当导数为零时。当导数为零时，点$x$为函数$f$可能的极值点/鞍点。

**单变量函数的二阶导：**

$$
    f''(x)= \frac{d^2f}{dx^2}=\lim_{h\to{0}}\frac{f'(x+h) - f'(x)}{h}
$$

二阶导数为导数在某点的变化率。


**1.1.2 常用公式**

**求导法则**

- 常量法则：$f'=0$; $f(x)$为常量；
- 和法则：$(af+\beta{g})' = af' + \beta{g'}$;
- 乘法则：$(fg)' = f'g + fg'$;
- 商法则：$(\frac{f}{g})' = \frac{f'g - fg'}{g^2} $
- 链式法则：若$f(x) = h(g(x))$,则$f'(x) = h'(g(x))g'(x)$

**基本函数求导法则**

- 幂函数的导数： $(x^r)' = rx^{r-1},r{\in}{\mathbb{R}}$
- 指数函数的导数：$(e^x)' = e^x,(a^x)' = a^xlna$. 注：$a^x = (e^{lina})^x = e^{xln(a)}$
- 对数函数的导数：$(lnx)' = \frac{1}{x},(log_{a}x)' = \frac{1}{xlna}$

### 1.2 偏导数

本部分回顾偏导数的定义及Hessian矩阵

**1.2.1 偏导数定义**

多变量函数的偏导数定义和单变量函数的导数定义类似，在某个变量的偏导数时固定其他变量，因此多变量函数的偏导数的数学记号如下：

$$
  \frac{\partial{f}}{\partial{x_i}} = \lim_{h\to{0}}\frac{f(x_i,...,x_i+h,...,x_n) - f(x_i,...,x_i,...,x_n)}{h}   (1.3)
$$

 当点 $x$ 处的所有偏导数为零时，点 $x$ 为函数 $f$ 可能的机数点/鞍点。其中

 $$ 
 x = \left[
\begin{matrix}
x_1\\
x_2 \\
\vdots\\
x_b
\end{matrix}
\right]
\tag{2}
$$

## 2. 矩阵论


### 2.1 矩阵基本知识

**张量(Tensor)：**是矢量概念的推广，可用来表示在一些矢量、标量和其他张量之间的线性关系的多线性函数。标量是0阶张量，矢量是一阶张量，矩阵是二阶张量，三维及以上数组一般称为张量。

**矩阵(Matrix)：** 是一个二维数组，其中的每一个元素一般由两个索引来确定一般用大写变量表示，m行n列的实数矩阵，记做$A{\in}{\mathbb{R}}_{m×n} $

**矩阵的秩(Rank)[^1]:**矩阵列向量中的极大线性无关组的数目，记作矩阵的列秩，同样可以定义行秩。行秩=列秩=矩阵的秩，通常记作rank(A)。

**矩阵的逆：** 
-  若矩阵A为方阵，当$rank(A_{n×n}) < n$时，称A为奇异矩阵或不可逆矩阵；
-  若矩阵A为方阵，当$rank(A_{n×n}) = n$时，称A为非奇异矩阵或可逆矩阵；

**逆矩阵知识点**

1. 逆矩阵公式有：$AA^{-1} = I = A^{-1}A$ ，其中 A 为 n 阶方阵 ， I 为N阶单位矩阵
2. A的逆矩阵的逆矩阵还是其本身：$(A^{-1})^{-1} = A$；
3. 若 A 和 B 都可逆，则 AB 也可逆，且其逆矩阵为 A 和 B 交换顺序后的逆的乘积：$(AB)^{-1} = B^{-1} . A^{-1}$；
4. 若 A 可逆，则 A 的转置 $A^T$ 也可逆，且其逆是 $A^{-1}$ 的转置：$(A^T)^{-1} = (A^{-1})^T$；可推导得：若干个可逆方阵，它们的乘积也可逆，该逆矩阵等于这些方阵按相反顺序相乘时的逆矩阵的乘积。 
    
**广义逆矩阵**

- 如果矩阵不为方阵或者是奇异矩阵，不存在逆矩阵，但是可以计算其广义逆矩阵或者伪逆矩阵；
- 对于矩阵A，如果存在矩阵 BB 使得 ABA=AABA=A，则称 BB 为 AA 的广义逆矩阵。


### 2.2 矩阵分解

  **特征值和特征向量**
  
  给定方阵 $A ∈ R_{n×n}$，我们称 $λ ∈ R$ 为该矩阵的特征值，$x ∈ R_n$ 为相应的特征向量，
若其满足如下公式:

  $$Ax=\lambda x ,x ≠ 0. $$
  

直观上看，该定义意味着对向量 $x$ 乘以矩阵 $A$ 生成一个新的向量，该向量和向量 $x$ 指向同一个方向，只是对向量采用因子 $λ$ 进行了缩放。同时也可以看到，对任意特征向量
$x ∈ R_n$，及标量 $c ∈ R$，有 $A(cx) = cAx = cλx = λ(cx)$，因此 $cx$ 也是特征向量。

- **矩阵特征分解：**[^2][^3] $ A = U{\sum}U^T $
- **矩阵奇异值分解：** $A=UΣV^T , U^TU=V^TV=I$

## 3. 概率论

### 3.1 随机变量

**随机变量(Random variable)：** 随机事件的数量表现，随机事件数量化的好处是可以用数学分析的方法来研究随机现象。

1. 离散随机变量: 指拥有有限个或者可列无限多个状态的随机变量
2. 连续随机变量: 指变量值不可随机列举出来的随机变量，一般取实数值。

举例：

1. 投掷一枚硬币为正面是离散型随机事件X，发生概率$P(X=1)=0.5$
2. 每次射箭距离靶心的距离X可以认为连续型随机变量，距离靶心小于1cm的概率$P(X<1cm)$


### 3.2 常见的概率分布

**伯努利分布：**

- 伯努利试验：只可能有两种结果的单次随机实验
- 又称0-1分布，单个二值型离散随机变量的分布
- 其概率分布：$P*(*X=1)=p,P(X=0)=1-pP∗(∗X=1)=p,P(X=0)=1−p.$


**二项分布**

- 二项分布即重复n次伯努利试验，各试验之间都相互独立
- 如果每次试验时，事件发生的概率为p，不发生的概率为1-p，则n次重复独立试验中事件发生k次的概率为
$$
P(X=k)=C^n_kp^k(1−p)^{n−k}
$$

**均匀分布**

均匀分布，又称矩形分布，在给定长度间隔[a,b]内的分布概率是等可能的，均匀分布由参数a，b定义，概率密度函数为：

$$
p(x)= \frac{1}{b−a},a<x<b
$$

**高斯分布**

高斯分布，又称正态分布(normal)，是实数中最常用的分布，由均值μ和标准差σ决定其分布，概率密度函数为：

$$
p(x)= \frac{1}{\sqrt{2πσ}}e^{-\frac{(x−μ)}{2σ^2}}
$$

**指数分布**

常用来表示独立随机事件发生的时间间隔，参数为λ>0的指数分布概率密度函数为：$p(x) = \lambda e^{-\lambda x} \quad x \geq 0
 x≥0.$ 指数分布重要特征是无记忆性。


### 3.3 多变量概率分布

**条件概率(Conditional probability)：**事件X在事件Y发生的条件下发生的概率，$P(X|Y)$

**联合概率(Joint probability)：**表示两个事件X和Y共同发生的概率，$P(X,Y)$

条件概率和联合概率的性质: $P(Y|X) = \frac{P(Y,X)}{P(X)} \quad P(X ) > 0$

推广到 n 个事件，条件概率的链式法则：

$$
P(X_1,X_2 ,…,X_n)=P(X_1∣X_2 ,…,X_n)P(X_2∣X_3,X_4,…,X_n)…P(X_{n−1}∣X_n)P(X_n)
=P(X_n)∏^{n−1}_{i=1}P(X_i∣X_{i+1},…,X_n)
$$


**先验概率(Prior probability)：**根据以往经验和分析得到的概率，在事件发生前已知，它往往作为“由因求果”问题中的“因”出现。

**后验概率(Posterior probability)：**指得到“结果”的信息后重新修正的概率，是“执果寻因”问题中 的“因”，后验概率是基于新的信息，修正后来的先验概率所获得 的更接近实际情况的概率估计。


全概率公式：设事件${A_i}$是样本空间 $Ω$ 的一个划分，且$P(A_i)>0(i=1,2,...,n)$，那么：$P(B) = \sum_{i = 1}^nP(A_i)P(B|A_i)$
 
 贝叶斯公式：全概率公式给我们提供了计算后验概率的途径，即贝叶斯公式
 

$$P\left(\mathrm{~A}_{i} \mid \mathrm{B}\right)=\frac{P\left(\mathrm{~B} \mid \mathrm{A}_{i}\right) P\left(\mathrm{~A}_{i}\right)}{P(\mathrm{~B})}=\frac{P\left(\mathrm{~B} \mid \mathrm{A}_{i}\right) P\left(\mathrm{~A}_{i}\right)}{\sum_{j=1}^{n} P\left(\mathrm{~A}_{j}\right) P\left(\mathrm{~B} \mid \mathrm{A}_{j}\right)}
$$
### 3.4 常用统计量

方差：用来衡量随机变量与数学期望之间的偏离程度。统计中的方差则为样本方差，是各个样本数据分别与其平均数之差 的平方和的平均数，计算过程为：

$\operatorname{Var}(X)=E\left\{[x-E(x)]^{2}\right\}=E\left(x^{2}\right)-[E(x)]^{2}
$
 
协方差：衡量两个随机变量X和Y直接的总体误差，计算过程为：

$\operatorname{Cov}(X,Y)=E\left\{[x-E(x)][y-E(y)]\right\}=E\left(xy\right)-E(x)E(y)
$


## 4. 信息论

### 4.1 熵(Entropy)

信息熵，可以看作是样本集合纯度一种指标，也可以认为是样本集合包含的平均信息量。

假定当前样本集合X中第i类样本 $𝑥_𝑖$ 所占的比例为$P(𝑥_𝑖)(i=1,2,...,n)$，则X的信息熵定义为：
$$
H(X) = -\sum_{i = 1}^n P(x_i)\log_2P(x_i)
$$
H(X)的值越小，则X的纯度越高，蕴含的不确定性越少

### 4.2 联合熵

两个随机变量X和Y的联合分布可以形成联合熵，度量二维随机变量XY的不确定性：

$H(X, Y) = -\sum_{i = 1}^n \sum_{j = 1}^n P(x_i,y_j)\log_2 P(x_i,y_j)$
 

### 4.3 条件熵

在随机变量X发生的前提下，随机变量Y发生带来的熵，定义为Y的条件熵，用H(Y|X)表示，定义为：
$$
H(Y|X) = \sum_{i = 1}^n P(x_i)H(Y|X = x_i) = -\sum_{i = 1}^n P(x_i) \sum_{j = 1}^n P(y_j|x_i)\log_2 P(y_j|x_i) = -\sum_{i = 1}^n \sum_{j = 1}^n P(x_i,y_j) \log_2 P(y_j|x_i)
$$
条件熵用来衡量在已知随机变量X的条件下，随机变量Y的不确定。 熵、联合熵和条件熵之间的关系：$H(Y|X) = H(X,Y)-H(X)$

### 4.4 互信息

### 4.5 相对熵


相对熵又称KL散度，是描述两个概率分布P和Q差异的一种方法，记做D(P||Q)。在信息论中，D(P||Q)表示用概率分布Q来拟合真实分布P时，产生的信息表达的损耗，其中P表示信源的真实分布，Q表示P的近似分布。

- 离散形式：$D(P||Q) = \sum P(x)\log \frac{P(x)}{Q(x)}$
- 连续形式：$D(P||Q) = \int P(x)\log \frac{P(x)}{Q(x)}$
### 4.6 交叉熵

一般用来求目标与预测值之间的差距，深度学习中经常用到的一类损失函数度量，比如在对抗生成网络( GAN )中

$$D(P||Q) = \sum P(x)\log \frac{P(x)}{Q(x)} = \sum P(x)\log P(x) - \sum P(x)\log Q(x) =-H(P(x)) -\sum P(x)\log Q(x)$$
交叉熵：$H(P,Q) = -\sum P(x)\log Q(x)$


## 5. 最优化估计

### 5.1 最小二乘估计

最小二乘估计又称最小平方法，是一种数学优化方法。它通过最小化误差的平方和寻找数据的最佳函数匹配。最小二乘法经常应用于回归问题，可以方便地求得未知参数，比如曲线拟合、最小化能量或者最大化熵等问题。


## 参考资料

1. [^1][矩阵的秩](https://docs.irudder.me/Linear-algebra-docs/ch0206.html)
2. [^2][矩阵特征分解](https://zhuanlan.zhihu.com/p/314464267)
3. [^3][奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)


```python

```
