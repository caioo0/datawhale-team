问题目录：  
1、决策树的实现、ID3、C4.5、CART（贝壳）  
2、CART回归树是怎么实现的？（贝壳）    
3、CART分类树和ID3以及C4.5有什么区别（贝壳）    
4、剪枝有哪几种方式（贝壳）  
5、树集成模型有哪几种实现方式？（贝壳）boosting和bagging的区别是什么？（知乎、阿里）  
6、随机森林的随机体现在哪些方面（贝壳、阿里） 
7、AdaBoost是如何改变样本权重，GBDT分类树的基模型是？（贝壳）     
8、gbdt,xgboost,lgbm的区别(百度、滴滴、阿里，头条)     
9、bagging为什么能减小方差？（知乎）      

其他问题：   
10、关于AUC的另一种解释：是挑选一个正样本和一个负样本，正样本排在负样本前面的概率？如何理解？   
11、校招是集中时间刷题好，还是每天刷一点好呢？    
12、现在推荐在工业界基本都用match+ranking的架构，但是学术界论文中的大多算法算是没有区分吗？end-to-end的方式，还是算是召回？  
13、内推刷简历严重么？没有实习经历，也没有牛逼的竞赛和论文，提前批有面试机会么？提前批影响正式批么？     
14、除了自己项目中的模型了解清楚，还需要准备哪些？看了群主的面经大概知道了一些，能否大致描述下？   

**1、决策树的实现、ID3、C4.5、CART（贝壳）**

**解答思路：**  

这道题主要是能写出算法的实现公式，我将大致从以下四个方面介绍每一个算法：思想、划分标准、剪枝策略，优缺点。

**1. ID3** 

ID3 算法建立在奥卡姆剃刀（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。

**1.1 思想**

从信息论的知识中我们知道：信息熵越大，从而样本纯度越低。 ID3 算法的核心思想是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。其大致步骤为：

1. 初始化特征集合和数据集合；    
2. 计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策结点；  
3. 更新信息集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；  
4. 重复2，3两步，若子集值包含单一特征，则为分支叶子节点。

**1.2 划分标准**

ID3 使用的分类标准是信息增益，它表示得知特征 A 的信息而使得样本集合不确定性减少的程度；
数据信息量：

$$
I(i) = - log_{2}\frac{|C_{i}|}{D}
$$

数据集的信息熵,可以理解为信息量的数学期望：

$$
H(D) = - \sum^{k}_{k=1}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{D}
$$



## 参考

- [1]. https://zhuanlan.zhihu.com/p/85731206    
- [2]. https://zhuanlan.zhihu.com/p/86263786    

