# 大数据技术学院-每日打卡3-6月完整整理

> 来源：知识星球 - 大数据技术学院

![image-20230627233057943](C:\Users\Jo_ch\AppData\Roaming\Typora\typora-user-images\image-20230627233057943.png)

##  #每日打卡# 3月24日面试题— HDFS的block为什么是128M？增大或减小有什么影响？

首先HDFS中存储数据是以块（block）的形式存放在DataNode中的，块（block）的大小可以通过设置dfs.blocksize来实现；

在Hadoop2.x的版本中，文件块的默认大小是128M，老版本中默认是64M；

寻址时间：HDFS中找到目标文件块（block）所需要的时间。

**原理：**

- 文件块越大，寻址时间越短，但磁盘传输时间越长；

- 文件块越小，寻址时间越长，但磁盘传输时间越短。

1. 如果块设置过大，

   一方面，从磁盘传输数据的时间会明显大于寻址时间，导致程序在处理这块数据时，变得非常慢；

   另一方面，mapreduce中的map任务通常一次只处理一个块中的数据，如果块过大运行速度也会很慢。

2. 如果块设置过小，

   一方面存放大量小文件会占用NameNode中大量内存来存储元数据，而NameNode的内存是有限的，不可取；

   另一方面文件块过小，寻址时间增大，导致程序一直在找block的开始位置。

为什么是128M?

1. HDFS中平均寻址时间大概为10ms；

2. 经过前人的大量测试发现，寻址时间为传输时间的1%时，为最佳状态；

   所以最佳传输时间为10ms/0.01=1000ms=1s

3. 目前磁盘的传输速率普遍为100MB/s；

   计算出最佳block大小：100MB/s x 1s = 100MB

   所以我们设定block大小为128MB。

ps：实际在工业生产中，磁盘传输速率为200MB/s时，一般设定block大小为256MB

 磁盘传输速率为400MB/s时，一般设定block大小为512MB

## \#每日打卡 3月27日面试题— MapReduce的mapper和reducer的个数如何确定？reducer的个数依据是什么？

1. 在 MapReduce 中，Mapper 和 Reducer 的个数都是由开发者根据实际需求来确定的。

- Mapper 的数量取决于输入数据的大小和处理器的数量，以及 MapReduce 框架的配置参数。

- Mapper 的数量应该越多越好，这样可以使得数据处理更加并行，从而提高处理速度。

2. Reducer 的数量则取决于 Mapper 的数量和数据处理的复杂度。

- Reducer 的作用是将 Mapper 处理过的数据进行汇总和整理，因此 Reducer 的数量应该相对于 Mapper 的数量要少一些。

- Reducer 的数量可以通过公式来计算：Reducer 数量 = Mapper 数量 / 配置参数中的 reduce.tasks 参数

- 其中，配置参数中的 reduce.tasks 参数是指在 Reducer 阶段中同时运行的任务数量。开发者可以根据实际需要来调整这个参数，以达到更好的性能和处理效果。
  

## \#每日打卡# 3月28日面试题— Hive的cluster by 、sort by、distribute by 、order by 区别？

Cluster by、sort by、distribute by 和 order by 都是 Hive 中用于对查询结果进行排序或分发的关键字，它们有以下区别：
Cluster by：按照指定的列对结果进行分组，并在每个分组内按照指定的列进行排序。clustery by 会将相同的值分到同一个 Reducer 任务中，从而实现对结果的排序。只支持在 Reducer 阶段进行排序。
Sort by：按照指定的列对结果进行排序。sort by 会将相同的值分到同一个 Mapper 任务中，从而实现对结果的排序。只支持在 Mapper 阶段进行排序。
Distribute by：按照指定的列对结果进行分发，并将相同的值分到同一个 Reducer 任务中。distribute by 不会进行排序，只是将结果分发到不同的 Reducer 任务中。
Order by：按照指定的列对结果进行排序，并且在 Reducer 阶段进行合并。order by 会将相同的值分到同一个 Reducer 任务中，并按照指定的列进行排序。在 Reducer 阶段，order by 会将每个 Reducer 任务的结果进行合并，并最终生成一个有序的结果集。
需要注意的是，由于 Hive 的查询优化机制，实际上并不建议在查询中使用 sort by 和 distribute by，因为它们可能会导致查询性能下降。在大多数情况下，使用 cluster by 和 order by 就可以实现所需的排序和分发功能。

## \#每日打卡# 3月29日面试题 Hive分区和分桶的区别

Hive 分区和分桶都是 Hive 中用于管理大数据的工具，但它们的作用和实现方式略有不同。
分区表是将一个大的数据集根据业务需要分割成小的数据集，每个数据集存放在一个独立的文件夹中。通过分区，可以提高查询效率，因为 Hive 可以通过 where 关键词选择指定分区，从而避免全表扫描。分区表对应的是 HDFS 上独立的文件夹，该文件夹存放的是该分区的所有数据，其实分区就是分目录。分区表的基本操作包括创建分区表、插入数据、查询分区表中的数据、增加/删除分区等。
分桶表是将数据根据某个或多个列的值进行划分，每个划分后的数据集合并成一个文件。通过分桶，可以提高查询效率，因为 Hive 可以通过 Clustered By 子句指定分桶的列，从而只需查询所需的数据。分桶表对应的是 HDFS 上的文件，该文件存放的是该分桶的所有数据，其实分桶就是合并后的文件。分桶表的基本操作包括创建分桶表、插入数据、查询分桶表中的数据等。
分区表和分桶表的区别在于：
分区表是将数据根据某个列的值进行分割，每个数据集存放在一个独立的文件夹中；而分桶表是将数据根据某个或多个列的值进行划分，每个划分后的数据集合并成一个文件。
分区表是通过 where 关键词选择指定分区，从而提高查找效率；而分桶表是通过 Clustered By 子句指定分桶的列，从而只需查询所需的数据。
分区表的基本操作包括创建分区表、插入数据、查询分区表中的数据、增加/删除分区等；而分桶表的基本操作包括创建分桶表、插入数据、查询分桶表中的数据等。

## \#每日打卡# 3月30日面试题— Hive count(distinct)有几个reduce，海量数据会有什么问题

在 Hive 中，使用 count(distinct) 语句时，如果多个字段需要进行去重计算，则会产生多个 Reduce 任务。每个 Reduce 任务都会将经过筛选的数据进行聚合，并输出最终结果。因此，如果数据量较大，使用 count(distinct) 可能会导致 Reduce 任务数量过多，从而引起性能问题。
具体来说，当数据量足够大时，使用 count(distinct) 可能会遇到以下问题：
Reduce 任务数量过多：由于每个字段都需要进行去重计算，因此会产生多个 Reduce 任务。这会导致 Job 的运行时间变长，同时也会增加资源消耗，如内存、CPU 等。
数据倾斜：由于每个 Reduce 任务都需要处理经过筛选的数据，因此如果数据分布不均，就会导致某些 Reduce 任务需要处理大量的数据，而其他 Reduce 任务则只需要处理少量的数据。这种情况下，部分 Reduce 任务可能会因为数据量过大而运行缓慢，从而导致整个 Job 运行缓慢。
计算资源不足：如果 Reduce 任务数量过多，就会导致计算资源不足。例如，如果集群中只有少量的 Reduce 任务可用，而 count(distinct) 生成的 Reduce 任务数量又很多，那么就会导致任务排队等待，从而使得 Job 运行时间变长。
因此，在处理海量数据时，如果需要使用 count(distinct) 语句进行去重计算，应该尽量避免使用多个字段进行计算，以减少 Reduce 任务的数量。如果必须使用多个字段进行计算，可以考虑使用其他方法，如 Group By 语句等，来减少 Reduce 任务的数量，提高 Job 的运行效率

## \#每日打卡# 3月31日面试题— Kafka作为消息队列，它可解决什么样的问题？

Kafka 是一款高性能、可扩展、高可用性的分布式消息队列系统，它可以解决以下问题：
数据异步处理：Kafka 可以将数据从生产者发送到消费者，从而实现异步处理。这使得系统可以更快地处理大量数据，同时还能减轻系统间的耦合度。
数据流式处理：Kafka 支持流式处理，这意味着消费者可以按顺序接收消息，而不必等待所有消息都到达后再进行处理。这使得系统可以更快地响应实时数据流。
数据持久化保证：Kafka 使用磁盘存储消息，即使在发生系统故障时，数据也不会丢失。此外，Kafka 还可以配置数据保留策略，以确保在特定时间段内或者达到特定大小后，数据会自动清理。
高吞吐量和低延迟：Kafka 在设计时就着重考虑了高吞吐量和低延迟，因此它可以处理大量的数据，同时保证消息能够快速地传递给消费者。
高可扩展性：Kafka 采用了分布式架构，因此可以很容易地扩展集群的规模。新增节点可以快速加入集群，从而提高消息处理能力。
高可用性：Kafka 采用了冗余设计，可以确保在节点故障时，集群仍能继续工作。此外，Kafka 还可以配置备份副本，以进一步加强数据的可靠性。
因此，Kafka 可解决数据异步处理、数据流式处理、数据持久化保证、高吞吐量和低延迟、高可扩展性、高可用性等问题。



## \#每日打卡# 4月1日面试题 Kafka相比于其它消息组件有什么好处？



Kafka 相比于其他消息组件具有以下好处：
高吞吐量：Kafka 旨在实现高吞吐量的数据传输，因此具有非常高的性能。它是一款分布式消息队列系统，能够支持大规模的数据存储和处理。
可扩展性：Kafka 采用了分布式架构，因此可以方便地扩展服务器数量来支持更多的数据流量。此外，Kafka 还支持数据备份，以确保系统的可用性和容错性。
实时性：Kafka 保证了数据的实时性，因为它采用了内存中的数据结构，可以将数据快速地写入和读取。此外，Kafka 还支持实时数据流处理，可以实时地分析和处理数据。
可靠性：Kafka 采用了分布式架构，因此可以保证数据的可靠性和持久性。此外，Kafka 还支持数据备份和容错机制，以确保系统的可用性和容错性。
易于使用：Kafka 具有简单的命令行界面，因此可以方便地进行安装、配置和使用。此外，Kafka 还提供了丰富的 API 和第三方工具，以便开发者能够方便地使用和管理 Kafka。

## \#每日打卡# 4月4日面试题—   Kafka的ISR机制

Kafka 的 ISR（In-Sync Replicas）机制是为了保证多副本（Replicas）数据一致性而设计的。在 Kafka 中，每个 Topic 的每个 Partition 都有多个副本，其中一个是 Leader，其余的是 Follower。Follower 副本通过定时从 Leader 副本中拉取最新数据，以确保与 Leader 副本数据一致。
为了确定副本数据与 Leader 数据是否一致，Kafka 使用了 ISR 机制。ISR 是一个副本列表，其中存储的都是能与 Leader 数据一致的副本。确定一个副本是否在 ISR 列表中，有以下两个判断条件：
根据副本和 Leader 的交互时间差，如果大于某个时间差（默认为 10 秒，配置参数为 rerplica.lag.time.max.ms），就认定这个副本不行了，将其从 ISR 中剔除。
根据 Leader 和副本的信息条数差值决定是否从 ISR 中剔除此副本，此信息条数差值根据配置参数 rerplica.lag.max.messages（默认为 4000）决定。
需要注意的是，Kafka 后续版本移除了第二个判断条件，只保留了第一个。因为极端情况下，如果 producer 一次性发来了 10000 条数据，默认条数差可能会立即大于 4000。

## \#每日打卡# 4月6日面试题— Kafka的ISR、OSR和ACK介绍，ACK分别有几种值？

Kafka 是一款高性能、跨语言的分布式 Publish/Subscribe 消息队列系统，其重要特性包括高吞吐、可持久化、可水平扩展、支持流数据处理等。Kafka 可以充当消息系统、存储系统和流式处理平台三大角色，支持消息顺序性保障及回溯消费等功能。
要查看 Kafka 运行状态，可以使用多种命令，如查看 topic 信息、指定 group 信息、重平衡、查看 kafkatopic 列表、查看 kafka 特定 topic 的详情、查看 consumergroup 列表等。此外，还有 Kafka 监控工具 kafka-monitor 可用于监控 Kafka 集群运行状态。
在 Kafka 中，ISR（In-Sync Replicas）是指与领导者同步的副本数量，OSR（Out-Of-Sync Replicas）是指与领导者不同步的副本数量，ACK（Acknowledgment）是指消息确认机制，用于确保消息被成功地写入 Kafka 并被消费者消费。ACK 有三种取值：0、1 和 2，分别表示未确认、成功确认和失败确认。

## \#每日打卡# 4月7日面试题— Kafka怎么保证数据不丢失，不重复？

Kafka 是一种高吞吐量的分布式消息队列系统，用于处理大规模的数据流。为了保证数据不丢失和不重复，Kafka 采用了多种机制。
首先，Kafka 使用分布式存储数据，将数据分散存储在多个服务器上，从而提高了数据的可靠性和容错性。如果某个服务器出现故障，数据可以自动切换到其他服务器上，从而保证数据的可靠性。
其次，Kafka 采用副本（Replication）机制，将每个主题（Topic）的分区（Partition）数据复制到多个副本中。这样可以增加数据的可靠性和容错性，如果某个副本出现故障，可以自动切换到其他副本上，从而保证数据的可靠性。
此外，Kafka 还采用了一些其他机制来保证数据的可靠性和顺序性，例如：
顺序性消费：Kafka 保证分区内的消息按照顺序消费，即先产生的消息先被消费。
唯一性：生产者可以设置消息的唯一性，即基于数据库的唯一键，通过消息内容拼成一个唯一的 key，然后创建一个幂等表进行判断，从而避免重复消息的产生。
确认机制：生产者在发送消息时可以设置 acks 参数，要求写入所有 ISR 中的副本成功后才认为是成功，从而保证数据的可靠性。
综上所述，Kafka 通过副本机制、顺序性消费、唯一性和确认机制等多种机制，保证了数据的可靠性和容错性，从而保证了数据的不丢失和不重复。

## #4月10日# 面试题— Kafka的消费者和消费者组有什么区别？为什么需要消费者组？

Kafka 的消费者和消费者组是 Kafka 中消息消费的核心概念。消费者是指用于消费 Kafka 主题中的消息的应用程序或进程，而消费者组是指一组消费者，它们共同消费同一个主题中的消息。消费者组中的消费者可以有不同的实例，它们可以消费主题的不同分区。
消费者组在 Kafka 中扮演着非常重要的角色，主要原因是 Kafka 采用了分组、分区的消费模式。Kafka 的主题被划分为多个分区，每个分区中的消息可以由多个消费者消费。消费者组可以帮助多个消费者共同消费同一个主题中的消息，从而提高消费者的吞吐量。同时，消费者组也可以实现消息的发布/订阅模式和点对点两种模式，使得 Kafka 在处理大规模数据时具有更高的性能和可靠性。
在 Kafka 中，消费者组是由一个或多个消费者组成的。消费者组中的消费者可以消费同一个主题中的不同分区，从而实现消息的并行处理。消费者组的成员可以动态增加或减少，从而灵活地适应不同的消费需求。此外，消费者组还支持负载均衡和故障转移等功能，从而提高了消费者的可靠性和容错性。
总之，消费者和消费者组是 Kafka 中消息消费的核心概念，它们共同协作可以实现高性能、高可靠性的消息处理。消费者组在 Kafka 中扮演着非常重要的角色，它可以帮助多个消费者共同消费同一个主题中的消息，从而提高消费者的吞吐量和可靠性。

## 4月11日面试题— Kafka的一个消费组的消费者越多其消费能力就越强吗？为了提高消费组的消费能力，是不是可以随意添加分区和消费者呢？

Kafka 的消费者组中的消费者数量越多，并不一定意味着消费能力越强。实际上，消费者组的消费能力主要取决于以下几个因素：
1. 分区数量：Kafka 中的数据是按照主题 (topic) 和分区 (partition) 进行组织的。每个分区是一个有序的、不可变的数据流，可以独立地进行生产和消费。因此，消费者组的消费能力取决于主题分区的数量。如果分区数量太少，那么消费者组可能无法充分利用 Kafka 的吞吐量和并行处理能力。
2. 消费者数量：消费者组中的消费者数量越多，可以同时处理的数据量也就越多。但是，如果消费者数量过多，可能会导致竞争条件和资源瓶颈，从而降低消费速度。此外，每个消费者都需要消耗系统资源，如内存、文件句柄和网络连接，因此消费者数量也需要适当控制。
3. 消费者配置：消费者组的每个消费者都需要配置适当的消费速率、缓存大小和重试次数等参数，以达到最佳的消费性能。如果配置不当，可能会导致消费速度过慢或过快，从而影响消费效果。
因此，为了提高消费者组的消费能力，应该根据实际情况适当增加分区数量和消费者数量，并进行性能测试和调优，以找到最佳的配置方案。同时，也应该注意消费者组的资源利用率和竞争条件，避免出现资源瓶颈和性能问题。

## 4月12日面试题— Kafka为什么同一个消费者组的消费者不能消费相同的分区？

Kafka 是一个高性能、可扩展、高可靠性的分布式消息队列系统，它允许多个消费者组成的消费者组同时消费同一个 Topic 的数据。然而，在同一个消费者组内，不同的消费者不能消费相同的分区，这是因为 Kafka 的设计原则是尽可能地保证数据的顺序和一致性。
在 Kafka 中，每个 Topic 都被划分为多个分区，每个分区都是一个有序的、不可变的数据流。消费者可以指定自己要消费的分区，当多个消费者同时消费同一个分区时，Kafka 会保证数据的顺序和一致性。这是因为 Kafka 使用了一种称为“顺序器”的技术，顺序器可以确保在同一个分区内，所有消费者的消费顺序是相同的，从而避免了数据混乱和重复消费的问题。
因此，如果同一个消费者组内的多个消费者要消费相同的分区，它们必须使用不同的消费者实例来分别消费该分区，以确保每个实例只能消费自己的数据。如果多个消费者共享同一个实例来消费相同的分区，那么 Kafka 将无法保证数据的顺序和一致性，可能会导致数据重复或丢失。

## 4月13日面试题— Kafka实现高吞吐的原理？

Kafka 是一个高性能、可扩展的分布式消息系统，它能够实现高吞吐量的原因主要有以下几个方面：
1. 顺序写磁盘：Kafka 采用顺序写磁盘的方式，将数据写入磁盘，避免了磁盘寻址的开销。此外，Kafka 还使用操作系统提供的 PageCache 功能，将数据首先写入内存中的 PageCache，然后再异步地刷新到磁盘中，从而进一步提高了写入性能。
2. Zero-copy：Kafka 采用 Zero-copy 的方式，将从生产者发出的数据直接复制到消费者所在的内存中，避免了多次数据复制的开销。Zero-copy 使得 Kafka 在高吞吐量的情况下，仍然能够保持低延迟。
3. 分布式架构：Kafka 采用分布式架构，将数据分散存储在多个服务器上，并通过负载均衡算法来保证数据的均衡分布。当某个服务器的负载过高时，Kafka 会自动地将部分数据迁移到其他服务器上，从而提高了整个系统的可扩展性和吞吐量。
4. 高性能序列化和反序列化：Kafka 采用高性能的序列化和反序列化算法，将数据转换为二进制格式进行存储和传输。Kafka 支持多种序列化格式，如 JSON、Avro、Protocol Buffers 等，用户可以根据自己的需求选择最适合的序列化格式。
5. 消息压缩：Kafka 支持消息压缩，可以对消息进行压缩和解压缩，从而减少了网络传输的数据量，提高了吞吐量。Kafka 支持多种压缩算法，如 GZIP、Snappy、LZ4 等，用户可以根据自己的需求选择最适合的压缩算法。
综上所述，Kafka 实现高吞吐量的原理主要包括顺序写磁盘、Zero-copy、分布式架构、高性能序列化和反序列化以及消息压缩等方面。这些技术手段相互配合，共同保证了 Kafka 在高吞吐量的情况下，仍然能够保持低延迟和高可靠性。

## 4月14日面试题— Kafka的单播和多播 

Kafka 是一种高吞吐量的分布式消息队列系统，它可以实现消息的单播和多播。单播是指一条消息只能被某一个消费者消费，而多播则是指一条消息能够被多个消费者消费。
要实现消息的单播，只需要让这些消费者属于同一个消费者组即可。当生产者发送一条消息时，只有同一个消费者组下的一个消费者能收到消息。而要实现消息的多播，只需要保证这些消费者属于不同的消费者组。这样，同一条消息就能被不同消费者组下的多个消费者消费。
在 Kafka 中，每个消费者都属于一个特定的消费者组，通过消费者组就可以实现消息的单播和多播。单播和多播的实现是基于 Kafka 的消费者组机制，而不是广播机制。因为一条消息只能被 Kafka 同一个分组下某一个消费者消费，而不是所有消费者都能消费，所以从严格意义上来讲，Kafka 的消息传递模式不能算是广播模式。
Kafka 提供了高度可扩展的架构，可以支持大规模的数据存储和处理。它通过将数据分散存储在多个服务器上，实现了高可用性和容错性。同时，Kafka 还支持数据备份和恢复，可以保证数据的安全性和可靠性。因此，Kafka 被广泛应用于大数据、实时计算和日志收集等场景。

## 4月17日面试题— Kafka的一条message中包含了哪些信息？

Kafka 的一条 message 包含以下信息：
消息主题 (topic)：消息所属的逻辑分类，用于指定消息的路由和消费。
消息键 (key)：可选的元数据，用于唯一标识消息。
消息值 (value)：消息的内容，可以是字节数组、字符串或其他类型的数据。
消息时间戳 (timestamp)：消息产生的时间戳，用于确定消息的顺序和有效期限。
分区 (partition)：消息所属的分区，用于在多个 broker 之间分配消息负载和消费。
生产者 ID(producer ID)：生成消息的生产者的唯一标识符。
消费者组 ID(consumer group ID)：消费消息的消费者组的唯一标识符。
消息序列号 (message sequence number)：消息在生产者中的序列号，用于确保消息的顺序和唯一性。
消息校验和 (message checksum)：可选的校验和，用于检测消息在传输过程中的错误和丢失。
这些信息一起构成了 Kafka 消息的元数据和负载，使得 Kafka 能够实现高性能、可扩展、高可用的消息传递和处理。

## \#每日打卡# 4月18日面试题— Kafka在哪些地方会有选举过程，使用什么工具支持选举？

Kafka 在以下两个方面会有选举过程：
1. 控制器选举：在 Kafka 集群中，控制器负责管理所有分区和副本的状态。当多个 broker 启动后，它们会在 Zookeeper 创建 /controller 节点，并通过 Zookeeper 的协调框架来选举出控制器。哪个 broker 成功创建该节点，它就成为了控制器。其他竞争失败的 broker 会进行 watch 注册，以便在控制器出现故障时能够及时发现并进行切换。
2. 分区 leader 选举：Kafka 是一个多分区、多副本的消息服务，每个分区的多副本由一个 leader 和多个 follower 构成。leader 负责进行数据读写，并管理整个 follower 中存储的数据状态。当分区中的 leader 出现故障时，Kafka 会通过选举过程选出一个新的 leader。选举过程基于 Zookeeper 的协调框架，通过选举算法（如 FastPaxos、Raft 等）来选出新的 leader。
Kafka 在选举过程中使用的工具主要是 Zookeeper 和 Kafka 自身的协调框架。Zookeeper 提供了节点的唯一性和协调性，用于支持控制器和分区 leader 的选举。Kafka 自身的协调框架则负责实现选举算法和协调各个 broker 之间的状态。

## \#每日打卡# 4月19日面试题— 介绍下HBase

HBase 是一个分布式、可扩展、高性能的列式存储 NoSQL 数据库，它最初是基于 Google 的 Bigtable 设计开发的。HBase 主要用来存储非结构化数据，如网页、日志、消息等，它支持海量数据的存储和快速查找。
HBase 表的结构与关系型数据库不同，它不需要提前指定表名和字段，而是以列族为单位进行建表。列族是列的集合，一个列族中包含多个列。在建表时，只需指定表名和列族，而不需要指定具体的字段，这样可以大大增加数据的灵活性和可扩展性。
HBase 的表存储的是一行一行的数据，每一行由一个唯一的行键（rowkey）和多个列组成。行键是每一行的 ID，它是自动创建的，建表时不需要指定，也不需要创建。插入数据时，只需指定表名、行键和要插入的列族和列，就可以将数据插入到 HBase 表中。
与关系型数据库相比，HBase 具有更好的可扩展性和灵活性，可以支持 10 亿行百万列的大表，并且可以快速地查找和插入数据。因此，HBase 被广泛应用于大数据、云计算、物联网等领域。

## \#每日打卡# 4月20日面试题— HBase优缺点

HBase 是一款分布式、可扩展、高性能的列式存储数据库，它是基于 Google 的 Bigtable 算法实现的。HBase 适用于海量数据存储和实时读写操作，被广泛应用于金融、电信、互联网等行业。以下是 HBase 的优缺点：
优点：  
1. 可扩展性：HBase 采用分布式存储架构，可以轻松地增加或删除节点来扩展存储容量和处理能力。  
2. 高性能：HBase 使用高效的列式存储和压缩算法，可以实现快速的读写操作和数据查询。  
3. 实时性：HBase 支持实时读写操作，可以实时获取和更新数据，适用于需要低延迟和高吞吐量的业务场景。  
4. 数据安全：HBase 支持多种数据安全机制，包括数据加密、访问控制和数据备份等，可以保障数据的安全性和完整性。  
5. 适用于海量数据存储：HBase 可以存储数十亿条数据记录，适用于处理大规模数据的业务场景。
缺点：  
1. 学习曲线较陡峭：HBase 是一款复杂的分布式数据库，需要掌握分布式系统、大数据处理等相关知识，学习难度较大。  
2. 性能调优复杂：HBase 的性能调优需要对分布式系统、存储引擎、列式存储算法等进行深入了解和优化，需要一定的技术能力和经验。  
3. 数据一致性问题：在分布式系统中，HBase 需要保证数据的一致性和可用性，这需要采用复杂的算法和优化技术来实现，也增加了使用难度。  
4. 适用于读密集型场景：HBase 适合处理读密集型业务场景，而对于写密集型业务场景，可能不适合使用 HBase。  
5. 缺乏成熟的工具和生态系统：相比于其他关系型数据库和 NoSQL 数据库，HBase 的生态系统和工具还不够成熟和完善。
总的来说，HBase 是一款功能强大的分布式数据库，适用于处理大规模数据的业务场景，但是需要一定的技术能力和经验来使用和优化。

## \#每日打卡# 4月21日面试题— HBase读写数据流程

HBase 是一个分布式、可扩展、高性能的列式存储系统，它基于 Google 的 Bigtable 设计。HBase 主要用于存储非结构化和半结构化的数据，如 Web 应用程序中的数据。下面是 HBase 读写数据的流程：
1. 写入数据流程：  
  - 客户端通过 RegionServer 连接到 HBase 集群。  
  - 客户端访问对应的 RegionServer，读取 hbase:meta 表，并将其缓存到连接中作为 MetaCache。  
  - 客户端调用 Table 的 put 方法写入数据，解析 RowKey 并对照缓存的 MetaCache，查看具体写入的位置由哪个 RegionServer 负责，将 put 请求发送到对应的 RegionServer。  
  - RegionServer 将数据顺序写入（追加）到 HDFS 的 WAL 中，并告诉 WAL 这批写入数据在内存中，还没有写入 HBase 内存。  
  - 当 WAL 中的数据被刷新到 HBase 内存中时，客户端才能看到刚刚写入的数据。
2. 读取数据流程：  
  - 客户端通过 RegionServer 连接到 HBase 集群。  
  - 客户端访问对应的 RegionServer，获取 Table。  
  - 客户端使用 get 方法读取数据，指定行键和列族、列限定符、值等条件。  
  - RegionServer 根据行键定位到对应的 Region，再根据列族、列限定符、值等条件定位到具体的列。  
  - RegionServer 从对应的 Store 文件中读取数据，并将数据返回给客户端。
HBase 的写入数据流程中，首先将数据写入 WAL，然后由 WAL 将数据刷写到 HBase 内存中。这样可以保证数据的持久性和可靠性，即使在写入过程中发生故障，也可以通过 WAL 恢复数据。而读取数据时，可以直接从 HBase 内存中读取数据，提高读取性能。

## \#每日打卡# 4月23日面试题 HBase的读写缓存

HBase 主要使用的是 Google BigTable，一个高性能、高性能的列存储系统。
HBase 是一个分布式，高性能的列存储系统，它是一个分布式、可扩展的分布式数据存储系统，适用于处理大数据量的实时性且可扩展。此外，为了提高效地解决分布式存储。它适合处理大型、高吞吐而不喊你拉屎，实现大吞吐蕃值。

## \#每日打卡# 4月24日面试题 在HBase中执行删除操作时，是否立马把数据删除掉？如果不是，那它什么时候真正的进行删除？

在 HBase 中，执行删除操作并不会立即将数据删除掉，而是将删除标记添加到表的元数据中。具体来说，当执行删除操作时，HBase 会在表的元数据中添加一个删除标记，表示该行已被删除。然后，HBase 会等待下次 Compaction 时进行删除操作。
在 HBase 中，Compaction 是一种机制，用于合并多个 MemStore（内存存储器）中的数据，并将其写入磁盘上的 HFile（HBase 文件）。当 Compaction 发生时，HBase 会检查表的元数据，如果发现某行已被删除，则会将该行的数据从 Compaction 的输出中删除。这样，该行数据就被真正地删除了。
需要注意的是，删除操作可能会导致数据丢失。例如，如果删除一个行时，该行是表的主键，那么整个表都会被删除。因此，在进行删除操作时，需要谨慎操作，并确保了解了删除操作的影响。

## \#每日打卡# 4月25日面试题— HBase的RegionServer宕机以后怎么恢复？

当 HBase 的 RegionServer 宕机时，HBase 会自动进行故障转移，将该 RegionServer 所负责的区域数据转移到其他可用的 RegionServer 上。这个过程是由 HBase 的负载均衡器（Load Balancer）来完成的。负载均衡器会监控所有 RegionServer 的状态，当发现一个 RegionServer 宕机时，它会将该 RegionServer 所负责的区域数据转移到其他可用的 RegionServer 上，以确保数据的可用性和一致性。
在故障转移过程中，负载均衡器会根据 RegionServer 的优先级来选择新的 RegionServer。优先级高的 RegionServer 会更先被选择，以确保数据的可靠性和可用性。当新的 RegionServer 被选择后，它会接管该 RegionServer 所负责的区域数据，并继续提供读写服务。
在 RegionServer 宕机的情况下，HBase 会自动进行故障转移，并将数据转移到其他可用的 RegionServer 上，以确保数据的可用性和一致性。

## \#每日打卡# 4月26日面试题— HBase的一个region由哪些东西组成？

在 HBase 中，一个 Region 由以下组件组成：
1. RegionServer：RegionServer 是一个 HBase 集群中的服务器，它负责管理一个或多个 Region。每个 RegionServer 都有自己的 Region，它们是由 HMaster 分配的。RegionServer 负责将数据存储在本地磁盘上，并通过 HBase 的客户端 API 提供对数据的访问。
2. Region：Region 是 HBase 中的数据分区，它由一个或多个 RegionServer 管理。每个 Region 都由一个起始行键和结束行键组成，其中起始行键是该 Region 中最小行的行键，结束行键是该 Region 中最大行的行键。Region 包含一个或多个 Store，每个 Store 都负责存储一部分数据。
3. Store：Store 是 HBase 中的数据存储单元，它负责存储一个或多个列族的数据。每个 Store 都有一个列族，一个列族可以包含一个或多个列。Store 将数据存储在磁盘上，并通过 HBase 的客户端 API 提供对数据的访问。
4. MemStore：MemStore 是 HBase 中的内存存储单元，它用于存储最新写入的数据。MemStore 是火山岩式的，即新数据写入时会覆盖旧数据。当 MemStore 达到一定大小时，它会被刷新到磁盘上，形成一个新的 Store。
5. Compaction：Compaction 是 HBase 中的数据压缩和清理过程，它用于清理过期的数据和删除标记。Compaction 可以清除过期的删除标记，并将多个 Store 合并成一个更大的 Store，以提高查询效率。
6. Region 协调器：Region 协调器负责管理 Region，包括创建、删除、分裂和合并 Region。当一个 Region 变得过大时，协调器会将其分裂成多个 Region，并将这些 Region 分配给不同的 RegionServer 管理。
总之，HBase 中的 Region 是一个重要的概念，它由多个组件组成，包括 RegionServer、Region、Store、MemStore、Compaction 和 Region 协调器。这些组件一起工作，使得 HBase 可以提供高性能、高可用性和高可扩展性的分布式存储服务。

## \#每日打卡#  4月27日面试题— HBase的rowkey设计原则

在 HBase 中，rowkey 是唯一标识一行记录的主键，它在查询中起着至关重要的作用。合理的 rowkey 设计对于 HBase 的性能和数据分布具有重要影响。以下是一些 HBase rowkey 设计原则：
1. 唯一性：rowkey 必须能够唯一地识别一行数据，这是最基本的原则。
2. 避免热点：要将经常一起读取的行存储放到一起，避免热点问题。可以通过反转（如时间戳反转）或组合多个字段（如用户 ID 和订单 ID）来实现。
3. 可读性和可解析性：rowkey 可以是任意字符串，但最好选择可读性和可解析性较好的字符串，以便在调试和故障排查过程中更容易理解和操作。
4. 短小精悍：rowkey 长度一般为 10-100bytes，最大长度为 64KB。较短的 rowkey 可以减少存储开销和提高查询性能。
5. 有序性：HBase 中的行是按照 rowkey 的字典顺序排序的，合理的 rowkey 设计可以优化 scan 操作。如果行顺序经常变化，可以考虑使用 Bloom Filter 等技术来提高查询性能。
6. 分隔符：如果 rowkey 由多个字段组成，可以使用分隔符（如下划线、冒号等）将它们分隔开。这样可以提高 rowkey 的可读性和可解析性，同时也方便在代码中进行拼接和解析。
总之，合理的 rowkey 设计对于优化 HBase 性能和避免热点问题至关重要。在实际应用中，需要根据具体业务场景和数据特点进行综合考虑和选择。

## \#每日打卡# 4月28日面试题— HBase的大合并、小合并是什么？

HBase 中的大合并和小合并是两种不同的合并操作，用于将多个 HBase 表合并成一个表或者将一个表分成多个表。
大合并是指将多个 HBase 表按照一定的规则合并成一个表。在大合并中，合并的表必须具有相同的表结构和相同的列族，并且列族下的列必须具有相同的名字和数据类型。大合并可以通过 HBase 自带的合并工具进行实现，例如使用“hadoop dfs -getmerge”命令将多个表合并成一个表。
小合并是指将一个 HBase 表分成多个表，通常是为了缩小表的大小，提高查询效率。在小合并中，合并的表必须具有相同的列族，并且列族下的列必须具有相同的名字和数据类型。小合并可以通过 HBase 自带的合并工具进行实现，例如使用“hadoop hbase merge”命令将一个表分成多个表。
需要注意的是，在进行大合并或小合并操作前，需要先对参与合并的表进行扫描，以获取表的元数据信息，包括表名、列族、列等信息。此外，在进行合并操作时，还需要指定合并的关键字，用于标识合并后的表。

## \#每日打卡# 5月4日面试题— HBase为什么随机查询很快？

HBase 是一个分布式、可扩展、高性能的列式数据库，它的设计旨在支持大规模数据的随机查询。以下是 HBase 随机查询很快的一些原因：
1. 列式存储：HBase 采用列式存储方式，将数据按照列族和列进行存储，而不是按照行存储。这种存储方式使得随机查询变得更加高效，因为查询时只需要获取所需的列数据，而不是整个行数据。
2. 数据分布：HBase 将数据分布在多个 RegionServer 上，每个 RegionServer 负责存储一部分数据。这种分布式存储方式使得随机查询可以分布在多个节点上并行执行，从而提高了查询效率。
3. MemStore：HBase 将新写入的数据先存储在内存中的 MemStore 中，然后定期将 MemStore 中的数据刷写到磁盘上。因为 MemStore 是内存中的存储，所以随机查询时通常会在内存中找到所需的数据，从而减少了磁盘 I/O 操作，提高了查询效率。
4. Bloom Filter：HBase 支持 Bloom Filter，它是一种数据结构，用于表示某个范围内的数据是否存在。通过 Bloom Filter，HBase 可以快速判断某个数据是否存在，从而避免了不必要的磁盘 I/O 操作。
5. 自动分区和负载均衡：HBase 支持自动分区和负载均衡，可以将数据自动分布到多个 RegionServer 上，并确保每个 RegionServer 上的负载均衡。这种机制使得随机查询可以更加高效地执行。
综上所述，HBase 的列式存储、数据分布、MemStore、Bloom Filter 和自动分区等特性使得随机查询非常快速和高效。

## \#每日打卡# 5月5日面试题— Spark的任务执行流程

Spark 的任务执行流程可以概括为以下几个主要步骤：
1. 创建 Spark 应用程序：在驱动程序中创建 Spark 配置对象和 Spark 应用程序，并通过 SparkContext 对象将任务集提交给 Spark 集群。
2. 划分任务集：DAGScheduler 根据任务集的宽窄依赖关系将任务集划分为一个个 Stage，每个 Stage 包含一个或多个并行计算任务。
3. 生成 TaskScheduler：为每个 Stage 生成一个 TaskScheduler，用于调度 Task 并在 Task 之间进行通信。
4. 提交 TaskSet：TaskScheduler 将 TaskSet 提交给 Executor，Executor 根据任务本地化算法将 Task 分配给不同的线程执行。
5. 执行 Task：在 Executor 中，Task 被反序列化成一个 TaskRunner 对象，然后由 ThreadPool 中的线程执行。
6. 通信：TaskScheduler 和 Executor 之间通过消息循环体进行通信，例如任务状态和结果的传递。
7. 整理结果：在所有 Task 执行完成后，TaskScheduler 将结果整理并返回给驱动程序。
以上是 Spark 任务执行的基本流程，具体的实现细节可以参考 Spark 源码。

## \#每日打卡# 5月6日面试题— Spark为什么适合迭代处理？

Spark 适合迭代处理的主要原因是其采用了基于内存的数据集表示和迭代运算机制。Spark 中的核心数据结构 RDD（弹性分布式数据集）以分区为单位进行计算，每个分区可以独立地进行计算和存储，这样可以减少磁盘 I/O 和网络传输的开销，提高计算效率。同时，Spark 还采用了迭代运算机制，通过在内存中存储中间计算结果，减少了迭代运算的磁盘 I/O 开销，并通过并行计算优化图的优化，降低延迟等待时间。
在迭代处理中，Spark 可以将一个计算任务分解为多个迭代步骤，每个步骤都会生成一个新的 RDD，并将其传递给下一个迭代步骤。这种方式可以有效地减少数据集的传输和处理时间，提高迭代处理的效率。此外，Spark 还支持多种编程模型，如 Scala、Python 和 Java，可以方便地开发和扩展迭代处理任务。
因此，Spark 适合迭代处理的主要原因是其基于内存的数据集表示和迭代运算机制，以及其高效的计算和数据处理能力。这使得 Spark 在处理大规模数据集时具有更高的效率和更快的迭代速度，成为处理大规模数据的理想工具。

## \#每日打卡# 5月8日面试题— Spark数据倾斜问题，如何定位，解决方案

Spark 数据倾斜是指在 Spark 应用程序中，某些数据分区的数据量远远超过其他分区，导致任务执行时间过长，从而影响整个应用程序的性能。以下是一些常用的方法来解决数据倾斜问题：
1. 均衡数据分区：通过对数据进行重新分区，使每个分区的数据量尽可能均衡，从而避免数据倾斜。
2. 使用随机前缀或哈希分桶：对于某些容易导致数据倾斜的操作，如 groupByKey 或 reduceByKey，可以在键值对的键上添加随机前缀或哈希分桶，从而将数据均匀分布到不同的分区中。
3. 使用聚合操作代替 join 操作：在进行 join 操作时，如果其中一个表的数据量很大，容易导致数据倾斜。此时可以考虑使用聚合操作代替 join 操作，如使用 reduceByKey 或 aggregateByKey 进行聚合。
4. 使用广播变量：对于一些小数据集，可以将其广播到所有节点上，避免在每个节点上重复加载数据，从而减少数据倾斜。
以上是常用的解决数据倾斜的方法。要定位数据倾斜的问题，可以通过以下步骤进行：
1. 检查 Spark 应用程序的代码，找出可能导致数据倾斜的操作，如 groupByKey、reduceByKey 等。
2. 检查数据集的分区情况，通过计算每个分区的数据量，找出可能存在数据倾斜的分区。
3. 检查 Spark 应用程序的配置，优化配置以提高性能和效率，如调整并行度、内存使用等。
4. 使用 Spark 提供的调试工具，如 Spark SQL 的解释器，分析查询计划和数据流，找出可能存在数据倾斜的地方。
5. 尝试使用 Spark 的调试工具和日志信息，跟踪应用程序的执行过程，找出可能导致数据倾斜的原因。
一旦定位到数据倾斜的问题，可以采取上述解决方案中的一种或多种来解决数据倾斜问题，从而提高 Spark 应用程序的性能和效率。

## \#每日打卡# 5月9日面试题— Spark join在什么情况下会变成窄依赖？

在 Spark 中，join 操作通常会产生宽依赖，即一个 RDD 的每个 Partition 都可能依赖于另一个 RDD 的多个 Partition，需要进行 shuffle 操作。但有时候，join 也可以变成窄依赖，即一个 RDD 的每个 Partition 最多只依赖于另一个 RDD 的一个 Partition。
具体来说，Spark join 算子会变成窄依赖的情况如下：
1. 当 join 操作的两个 RDD 的 partitioner 相同时，且至少其中一个 RDD 是 HashPartitioner 时，join 操作可以变成窄依赖。这是因为 HashPartitioner 会对每个 key 分配一个固定的 partition，因此只有与该 key 相关的 Partition 需要进行 shuffle 操作，其他 Partition 则不需要。
2. 当 join 操作的两个 RDD 中，至少有一个 RDD 的每个 Partition 都只包含一个元素时，join 操作也可以变成窄依赖。这是因为在这种情况下，每个 Partition 只需要与另一个 RDD 中的一个 Partition 进行 shuffle 操作即可。
需要注意的是，窄依赖并不意味着 join 操作不需要进行 shuffle 操作，只是 shuffle 的范围变小了。在实际应用中，窄依赖的情况比较少见，通常情况下，join 操作仍需要进行宽依赖操作。

## \#每日打卡# 5月10日面试题— 介绍Spark中算子的作用，为什么Spark要把算子分为Transform 和Action？常用的算子列举一些

在 Spark 中，算子（Operator）是一种对 RDD 执行的操作，用于转换或执行 RDD 上的计算。Spark 将算子分为两类：Transform 和 Action。
Transform 算子用于转换 RDD 的内容，其结果是一个新的 RDD。Transform 算子可以串联使用，即在一个 RDD 上连续应用多个 Transform 算子，每次转换都会生成一个新的 RDD。例如，Spark 中常用的 Transform 算子包括：map、filter、reduce、groupByKey 等。
Action 算子用于执行计算操作，其结果不是 RDD。Action 算子通常用于对 RDD 进行计算，例如计算 RDD 的计数、聚合等。Action 算子不能串联使用，即在一个 RDD 上不能连续应用多个 Action 算子。例如，Spark 中常用的 Action 算子包括：count、collect、saveAsTextFile 等。
Spark 中常用的算子包括：
1. Transform 算子：map、filter、reduce、groupByKey、mapKey、mapValue、filterKey、filterValue、sample、take、drop 等。
2. Action 算子：count、collect、saveAsTextFile、saveAsObjectFile、union、join、cogroup、sample、take、drop 等。
这些算子都可以通过 Spark 的 API 进行使用，开发者可以根据不同的需求选择不同的算子来对 RDD 进行操作。

## \#每日打卡# 5月11日面试题— Spark的哪些算子会有shuffle过程？

在 Spark 中，会有 shuffle 过程的算子主要包括：
1. reduceByKey：该算子会对上一个 RDD 中的每个 key 对应的所有 value 进行聚合，生成一个新的 RDD，元素类型是<key,value>对的形式。在聚合之前，每个 key 对应的 value 可能分布在不同的 partition 中，因此需要进行 shuffle 操作。
2. groupByKey：该算子会将上一个 RDD 中的数据按照 key 进行分组，生成一个新的 RDD，元素类型是<key,values>对的形式，其中 values 是一个可迭代的集合，包含该 key 对应的所有 value。由于每个 key 对应的 value 可能分布在不同的 partition 中，因此需要进行 shuffle 操作。
3. join：该算子用于将两个 RDD 按照指定的列进行连接。在连接过程中，需要将两个 RDD 中的数据按照相同的分区规则进行 shuffle 操作，以便将属于同一 key 的数据连接在一起。
4. cogroup：该算子类似于 join，用于将多个 RDD 按照指定的列进行连接。在连接过程中，需要将多个 RDD 中的数据按照相同的分区规则进行 shuffle 操作，以便将属于同一 key 的数据连接在一起。
5. mapReduce：该算子会对上一个 RDD 中的每个元素进行 map 操作，然后将结果进行 reduce 操作。在 reduce 操作之前，需要将每个 key 对应的 value 进行聚合，因此需要进行 shuffle 操作。
6. filter：该算子会对上一个 RDD 中的每个元素进行过滤操作。在过滤操作之前，需要将每个 key 对应的 value 进行聚合，因此需要进行 shuffle 操作。
需要注意的是，shuffle 操作是一种比较耗时的操作，可能会导致性能瓶颈。因此，在实际使用中，应该尽量避免使用需要进行 shuffle 操作的算子，或者尽可能减少 shuffle 操作的次数。

## \#每日打卡# 5月12日面试题— Spark的RDD、DataFrame、DataSet、DataStream区别？

Spark 是一个用于大规模数据处理的开源分布式计算框架，它提供了多种数据结构和运算符来支持各种数据处理任务。其中最常用的数据结构包括 RDD（弹性分布式数据集）、DataFrame、DataSet 和 DataStream。这些数据结构之间有一些相似之处，但也有一些关键的区别。
1. RDD（弹性分布式数据集）
RDD 是 Spark 中最基本的数据结构，它是一个不可变的、分布式的数据集合，由一组元素组成，每个元素都是一个字节序列。RDD 可以通过多种方式进行分区和重新分区，以实现高度并行化的数据处理。RDD 提供了丰富的转换操作（如 map、filter、reduce 等）和行动操作（如 count、collect 等），以支持各种数据处理任务。
2. DataFrame
DataFrame 是 Spark 中一种结构化数据表示方式，它类似于关系型数据库中的表。DataFrame 具有明确定义的列和数据类型，可以进行查询、过滤、排序、聚合等操作。DataFrame 可以通过 Spark 的 SQL API 进行操作，支持多种查询方式，如筛选、投影、连接、聚合等。
3. DataSet
DataSet 是 Spark 中另一种结构化数据表示方式，类似于 DataFrame，但它更加灵活。DataSet 可以定义为 RDD 上的一个逻辑查询，可以包含多个数据流（如实时数据或批处理数据），并且可以进行多种转换和行动操作。DataSet 可以通过 Spark 的 DSL API 进行操作，支持多种查询方式，如筛选、投影、连接、聚合等。
4. DataStream
DataStream 是 Spark 中一种用于实时数据处理的数据结构，它可以处理实时数据流，如传感器数据、消息数据等。DataStream 与 DataFrame 和 DataSet 类似，也可以进行查询、过滤、排序、聚合等操作。不同的是，DataStream 可以处理实时数据，可以与 Spark Streaming 等其他实时数据处理框架集成。
总结起来，RDD 是 Spark 中最基本的数据结构，提供了丰富的转换和行动操作，但比较原始和不结构化；DataFrame 和 DataSet 是 Spark 中的结构化数据表示方式，提供了丰富的查询和操作功能，但 DataFrame 注重结构化数据的处理，而 DataSet 更加灵活，可以处理多种数据流；DataStream 则是 Spark 中用于实时数据处理的数据结构。

## \#每日打卡# 5月15日面试题— 介绍下Spark Shuffle

Spark Shuffle 是一种在 Spark 中执行数据重分布的机制，它可以在 Spark 应用程序中的不同阶段之间传输数据。Shuffle 是在 Spark 中执行批处理的核心组件之一，它可以在多个 Map 阶段之后，将数据按照 Key 进行分组，并将这些数据传输到 Reduce 阶段进行处理。
Spark Shuffle 与 Hadoop Shuffle（MapReduce）有相似之处，但它们之间也存在一些差异。从逻辑角度来讲，Shuffle 过程就是一个 GroupByKey 的过程，两者没有本质区别。但是，从数据流角度讲，Spark 可以从多个 MapStageshuffle 数据，而 MapReduce 只能从一个 MapStage shuffle 数据。此外，Shuffle write/read实现上有一些区别，以前的分类是sort-based和hash-based。MapReduce可以说是sort-based，shuffle write 和 shuffle read 过程都是基于 keys sorting 的。但目前的 Spark 是两者的结合体，shuffle write 可以是 sort-based（只是 sort partition id，无需 keys sorting），shuffle read 阶段可以是 hash-based。因此，目前 sort-based 和 hash-based 已经你中有我，我中有你，界限已经不那么清晰。
另外，从数据 fetch 与数据计算的重叠粒度来讲，两者也有细微区别。MapReduce 是粗粒度，reducer fetch 到的 records 先被放到 shuffle buffer 中休息，当 shuffle buffer 快满时，才对它们进行 combine。而 Spark 是细粒度，可以立即对 fetch 到的数据进行计算。这种差异使得 Spark 在大数据处理方面具有更高的性能和效率。

## \#每日打卡# 5月16日面试题— Spark有了RDD，为什么还要有Dataform和DataSet？

Spark 中的 RDD（Resilient Distributed Dataset）是一个分布式的无序数据集合，它可以存储任何类型的数据，包括整数、字符串、JavaBean 等。RDD 通过将数据切分为多个分区，实现了数据的并行处理和分布式存储。然而，RDD 的 api 对于某些操作来说可能不太方便，例如对数据进行转换、过滤、聚合等操作时需要编写特定的函数，因此，Spark 还提供了 DataFrame 和 Dataset 接口。
DataFrame 和 Dataset 接口可以让用户以一种更结构化的方式处理数据。DataFrame 接口类似于关系型数据库中的表，它包含了数据的结构信息和数据本身，可以进行查询、过滤、聚合等操作。相比之下，RDD 接口更注重数据的分布式存储和计算，需要用户自己编写特定的函数来完成某些操作。
另外，DataFrame 和 Dataset 接口可以提高代码的可复用性和可维护性。例如，如果用户需要对一个包含姓名、年龄和性别的 DataFrame 进行过滤和聚合操作，可以使用一些通用的函数，如 filter、map、reduce 等，这些函数可以应用于多个列上，而不需要编写特定的函数。此外，DataFrame 和 Dataset 接口还可以自动处理数据的类型转换和缺失值处理等问题。
总的来说，DataFrame 和 Dataset 接口是 RDD 接口的扩展和补充，可以让用户更方便、更高效地处理数据。虽然 DataFrame 接口正在逐渐被 Dataset 接口所取代，但它们仍然都是 Spark 中重要的数据结构和 api。

## \#每日打卡# 5月17日面试题— Spark的Job、Stage、Task分别介绍下

Spark 的 Job、Stage、Task 是 Spark 框架中的核心概念，用于描述 Spark 应用程序的执行过程。下面是对这三个概念的详细介绍：
1. Job：Spark 中的 Job 是一个独立的计算任务，它包含了输入数据集和一组转换操作，以及一个或多个输出数据集。Job 是 Spark 应用程序的基本执行单元，每个 Job 都由一个或多个 Stage 组成。
2. Stage：Spark 中的 Stage 是 Job 中的一个计算阶段，它包含了一个或多个 Task，以及它们之间的数据依赖关系。每个 Stage 都有独立的输入数据集和输出数据集，以及一组转换操作。Spark 采用了基于阶段的 DAG（有向无环图）调度算法，以确保 Stage 之间的数据依赖关系得到满足。
3. Task：Spark 中的 Task 是 Stage 中的计算任务，它是 Spark 应用程序的最小执行单元。每个 Task 都会生成一个或多个输出数据集，并将其发送到下一个 Task 或输出到外部存储系统。Task 是 Spark 框架中的核心执行单元，Spark 采用了各种优化技术，如内存计算和代码生成，以提高 Task 的执行效率。
在 Spark 应用程序的执行过程中，Job 被分解成多个 Stage，每个 Stage 包含一个或多个 Task。Spark 的调度器会根据每个 Task 的依赖关系和资源需求，动态地分配资源并调度 Task 的执行。这种基于阶段的调度算法可以确保 Spark 应用程序的高性能和可扩展性。

## \#每日打卡# 5月18日面试题— Spark中的RDD的容错

在 Spark 中，RDD（弹性分布式数据集）是一种分布式数据结构，它可以切分为多个分区（partition），并在集群的不同节点上存储。RDD 的容错机制是通过“血统重建”（lineage）实现的，即通过记录 RDD 的转换操作历史来恢复丢失的分区。当某个节点在操作过程中崩溃时，Spark 会尝试分配另一个节点在 RDD 的相同分区上继续处理，以保证数据不丢失。此外，Spark 还提供了 RDD 检查点（checkpoint）机制，可以定期将 RDD 的数据和元数据保存到外部存储系统中，以便在故障时进行恢复。RDD 的容错机制使得 Spark 在处理大规模数据时具有高可用性和高可靠性，成为了一种流行的大数据处理框架。

## \#每日打卡# 5月19日面试题— 介绍下Spark的内存管理机制

Spark 是一个基于内存的大数据计算引擎，因此在编写 Spark 程序或提交 Spark 任务时，需要特别注意内存方面的优化和调优。Spark 官方提供了许多配置参数，用于进行内存或 CPU 的资源使用。这些参数的影响和作用，可以从 Spark 内存管理的原理方面进行分析。
Spark 任务最终是运行在 Java 虚拟机里面的，因此需要先了解 JVM 的内存区域划分。JVM 的运行时内存划分主要包括程序计数器、Java 栈、本地方法栈、方法区和堆。其中，堆是 Java 虚拟机管理内存最大的一块内存区域，也是影响性能的主要因素之一。堆内存溢出是 JAVA 项目非常常见的故障，因此必须了解 JAVA 堆内存是怎么工作的。
从 JVM 内存模型的角度来看堆内内存，又分为工作内存和主内存。工作内存属于线程私有，主内存数据线程公有。当线程公有的时候，如果线程 1 和线程 2 同时进行某个共享变量的读写，就会出现数据异常。因此，内存模型会涉及到可见性、原子性和指令重排序等概念。
Spark 的内存管理机制主要通过配置参数来实现。例如，可以通过设置 spark.memory.fraction 参数来控制 Spark 分配的内存比例。该参数的值应该在 0.6-0.8 之间，如果不够，Spark 将不得不使用磁盘存储中间数据，这将导致性能问题。另外，还可以通过设置 spark.memory.manager 参数来选择 Spark 的内存管理策略。可以选择使用堆内存管理器或自定义的内存管理器。
总之，Spark 的内存管理机制是通过配置参数来实现的，这些参数可以影响 Spark 任务的执行效率。了解 JVM 的内存区域划分和内存模型，可以帮助我们更好地理解 Spark 的内存管理机制。

## \#每日打卡# 5月22日面试题— Hive内部表和外部表的区别？

在 Hive 中，内部表和外部表是两种不同的表类型，它们之间的主要区别在于数据存储和管理的方式。
当 Hive 创建内部表时，它会将数据移动到数据仓库指向的路径。这意味着 Hive 会复制数据到内部的存储引擎中，以便对数据进行更好的管理和处理。因此，内部表的数据完全由 Hive 管理。
相比之下，当创建外部表时，Hive 仅记录数据所在的路径，而不会对数据的位置做任何改变。外部表的数据存储在 Hadoop 分布式文件系统（HDFS）上，而不是存储在 Hive 内部的存储引擎中。因此，外部表的数据不受 Hive 的管理，而是由 Hadoop 来管理。
当删除内部表时，Hive 会删除表的元数据和数据。但是，当删除外部表时，Hive 只会删除表的元数据，而不会删除数据。这是因为外部表的数据存储在 HDFS 上，而不是存储在 Hive 内部的存储引擎中。
内部表适合存储原始数据或比较重要的中间数据，以便进行表查询和分析等操作。而外部表则适合存储不需要进行频繁查询和分析的数据，例如日志数据等。
另外，Hive 还支持分区表，可以将数据按照某个维度进行分区存储，例如按照小时或天存储日志数据。这样可以针对某个特定时间段进行业务分析，而不必分析扫描所有数据。

## \#每日打卡# 5月23日面试题— 遇到过Hive数据倾斜问题吗，怎么排查及解决的？

作为一款基于 Hadoop 的数据库系统，Hive 在处理大规模数据时，可能会遇到数据倾斜问题。数据倾斜是指数据的 key 分布不均，导致部分任务需要处理大量数据，而其他任务则只需要处理少量数据，从而影响查询效率。
为了排查 Hive 数据倾斜问题，可以采取以下步骤：
确认是否存在数据倾斜：通过查询执行进度信息，观察任务的运行情况。如果大部分任务已经完成，但仍有少量任务在运行，可能存在数据倾斜。同时，可以通过分析表的数据分布情况，确认数据的 key 是否分布均匀。
确认数据倾斜的原因：数据倾斜可能由于数据的本身特点引起，例如某些 key 值出现的频率非常高，导致数据分布不均；也可能是由于查询语句的设计不合理，导致某些任务需要处理大量数据。
优化查询语句：针对数据倾斜的原因，可以对查询语句进行优化。例如，可以增加分区键，使数据分散到多个 Reducer 任务中；可以使用合适的聚合函数，如 SUM、AVG 等，避免对大量数据进行重复计算。
调整任务分配策略：可以通过调整 Hive 的配置参数，更改任务分配策略。例如，可以设置每个 Reducer 任务处理的最大数据量为一个固定的值，避免某个任务处理过多数据。
使用倒排索引：对于某些查询场景，可以使用倒排索引技术，提高查询效率。倒排索引可以将表按照某一列进行排序，并记录每行数据的位置，这样就可以避免在查询时重复读取数据。
综上所述，排查和解决 Hive 数据倾斜问题需要结合实际情况，采取不同的方法。通过优化查询语句、调整任务分配策略和使用倒排索引等技术，可以有效提高 Hive 的查询效率。



## \#每日打卡# 5月24日面试题— Hive的三种自定义函数是什么？它们之间的区别？

在 Hive 中，自定义函数可以根据其功能分为三种类型：UDF（User-Defined Function）、UDAF（User-Defined Aggregate Function）和 UDTF（User-Defined Table-Generating Function）。
1. UDF（User-Defined Function）：UDF 是最基本的自定义函数类型，它是一种单行函数，接收一个或多个输入参数，并返回一个输出值。UDF 函数在查询中作用于每一行数据，独立处理，不依赖于其他行的数据。例如，可以使用 UDF 函数实现字符串处理、数学计算等功能。
2. UDAF（User-Defined Aggregate Function）：UDAF 是一种聚合函数，它接收一列或多列数据作为输入，并返回一个聚合结果。UDAF 函数可以与 GROUP BY 子句结合使用，对数据进行分组和汇总计算。例如，可以使用 UDAF 函数实现求和、计数、平均值等聚合功能。
3. UDTF（User-Defined Table-Generating Function）：UDTF 是一种生成表的函数，它接收一行或多行数据作为输入，并返回一个或多个新行。UDTF 函数可以与 INSERT OVERWRITE 子句结合使用，将生成的数据插入到表中。例如，可以使用 UDTF 函数实现将一行数据拆分为多行、将 JSON 字符串解析为表格数据等功能。
这三种自定义函数之间的区别在于它们的功能和用途不同。UDF 函数主要用于单行数据处理，UDAF 函数主要用于聚合计算，UDTF 函数主要用于生成表格数据。在实际使用中，根据不同的业务需求和数据处理场景，可以选择不同类型的自定义函数来实现数据处理和分析。

## \#每日打卡# 5月25日面试题— Hive SQL转化为MR的过程？

Hive SQL 是一种用于查询和分析存储在 Hadoop 分布式文件系统 (HDFS) 上的数据的查询语言。Hive SQL 可以通过 ClientCLI、JDBC/ODBC、WEBUI 等方式进行使用。在 Hive 中，元数据存储在 Metastore 中，包括表名、表所属的数据库、表的拥有者、列/分区字段、表的类型、表的数据所在目录等信息。
当执行 Hive SQL 查询时，查询语句会被解析成抽象语法树 AST，然后对 AST 进行语法分析，检查表是否存在、字段是否存在、SQL 语义是否有误等。接下来，将 AST 编译成逻辑执行计划，然后使用优化器对逻辑执行计划进行优化。最后，将优化后的逻辑执行计划转换成可以运行的物理计划，通常是 MR（MapReduce）或 Spark 任务。
Hive SQL 转换为 MR 的过程可以概括为以下几个步骤：
1. 解析器（SQLParser）将 Hive SQL 查询语句解析成抽象语法树 AST。  
2. 编译器（PhysicalPlan）将 AST 编译成逻辑执行计划。  
3. 优化器（QueryOptimizer）对逻辑执行计划进行优化。  
4. 执行器（Execution）将逻辑执行计划转换成可以运行的物理计划，通常是 MR 或 Spark 任务。  
5. 生成最终的执行计划，并将任务提交到 Hadoop 集群运行。
在 MR 任务中，Hive SQL 查询语句会被转换成一个包含多个阶段的任务，每个阶段都由一个或多个 Mapper 和一个 Reducer 组成。在执行过程中，每个 Mapper 会处理输入数据中的一部分，并将结果输出到一个中间文件中，然后 Reducer 会将这些中间文件合并成最终结果。通过这种方式，Hive SQL 查询可以在 Hadoop 集群上并行执行，以提高查询效率。

## \#每日打卡# 5月26日面试题— Hive支持的计算引擎

目前 Hive 支持 MapReduce、Tez 和 Spark3 种计算引擎。
MapReduce 计算引擎
MapReduce 是 Hive 最早支持的计算引擎，但是在 Hive2.0 之后不推荐使用。Map 和 Reduce 的工作流程如下：Map 在读取数据时，将数据拆分成若干数据块，并读取到 Map 方法中被处理。数据在输出的时候，被分成若干分区并写入内存缓存（buffer）中，内存缓存被数据填充到一定程度会溢出到磁盘并排序。当 Map 执行完后会将一个机器上输出的临时文件进行归并存入到 HDFS 中。Reduce 启动时，会启动一个线程去读取 Map 输出的数据，并写入到启动 Reduce 机器的内存中，在数据溢出到磁盘时会对数据进行再次排序。当读取数据完成后会将临时文件进行合并，作为 Reduce 函数的数据源。
Tez 计算引擎
Apache Tez 是进行大规模数据处理且支持 DAG 作业的计算框架，它直接源于 MapReduce 框架，除了能够支持 MapReduce 特性，还支持新的作业形式，并允许不同类型的作业能够在一个集群中运行。Tez 将原有的 Map 和 Reduce 两个操作简化为一个概念——Vertex，并将原有的计算处理节点拆分成多个组成部分：VertexInput、VertexOutput、Sorting、Shuffling 和 Merging。计算节点之间的数据通信被统称为 Edge，这些分解后的元操作可以任意灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可形成一个大的 DAG 作业。通过允许 Apache Hive 运行复杂的 DAG 任务，Tez 可以用来处理数据，之前需要多个 MR jobs，现在一个 Tez 任务中。
Spark 计算引擎
Spark 是一个快速而通用的计算引擎，可以处理批量数据和流式数据。它支持多种编程语言，包括 Java、Python 和 Scala 等。Spark 提供了一个易用的编程模型，可以大大简化分布式计算应用程序的开发。Hive 也支持 Spark 作为计算引擎，可以使用 Spark 执行 Hive 查询，从而提高查询性能。 
总之，Hive 支持多种计算引擎，可以根据不同的业务场景和数据处理需求选择不同的计算引擎。

## \#每日打卡#  5月29 日面试题— Hive支持索引吗？

Hive 支持索引，但索引不是直接在 Hive 中创建的，而是通过在 Hive 中创建表时使用外部键或复合主键来实现的。
当创建一个表时，如果其中一个或多个列被指定为外部键或复合主键，则 Hive 会自动为这些列创建一个 B-Tree 索引。这个索引可以帮助 Hive 更快地查找和访问表中的数据。
要注意的是，在 Hive 中创建索引并不是像在传统的关系型数据库中那样通过创建一个单独的索引表来实现的。在 Hive 中，索引是与表紧密耦合的，因此索引的维护和查询优化都由 Hive 负责管理。
此外，Hive 还支持使用 Bloom Filter 和 Join Index 等技术来优化查询性能。这些技术可以帮助 Hive 更快地过滤和连接表中的数据，从而提高查询效率。

## \#每日打卡# 5月30日面试题— 使用Hive时做过哪些优化

在使用 Hive 时，可以采用以下优化方法：
1. Join 连接时的优化：当多个表进行查询时，从左到右表的大小顺序应该是从小到大。原因是 Hive 在对每行记录操作时会把其他表先缓存起来，直到扫描最后的表进行计算。同时在使用 ON 语句进行连接时，如果每个 ON 使用相同的字段连接时只会产生一个 MapReduce。
2. 在 where 字句中增加分区过滤器，可以减少扫描的数据量。
3. 当可以使用 leftsemijoin 语法时不要使用 innerjoin，前者效率更高。原因是对于左表中指定的一条记录，一旦在右表中找到立即停止扫描。
4. 如果所有表中有一张表足够小，则可置于内存中，这样在和其他表进行连接的时候就能完成匹配，省略掉 Reduce 过程。设置属性即可实现，sethive.auto.covert.join=true；用户可以配置希望被优化的小表的大小 sethive.mapjoin.smalltable.size=2500000；如果需要使用这两个配置可置入 HOME/.hiverc 文件中。
5. 同一种数据的多种处理：从一个数据源产生的多个数据聚合，无需每次聚合都需要重新扫描一次。例如：insertoverwritetablestudentselectfromemployee;insertoverwritetablepersonselectfromemployee；可以优化成：fromemployeeinsertoverwritetablestudentselectinsertoverwritetablepersonselect。
6. Limit 调优：Limit 语句通常是执行整个语句后返回部分结果。sethive.limit.optimize.enable=true；可以提高性能。
7. 开并发执行。某个 job 任务中可能包含众多的阶段，其中某些阶段没有依赖关系可以并发执行，开启并发执行后 job 任务可以更快的完成。设置属性：sethive.exec.parallel=true;。
8. Hive 提供的严格模式，禁止 3 种情况下的查询模式。a：当表为分区表时，where 字句后没有分区字段和限制时，不允许执行。b：当使用 orderby 语句时，必须使用 limit 字段，因为 orderby 只会产生一个 reduce 任务。c：限制笛卡尔积的查询。
9. 合理的设置 map 和 reduce 数量。
10. JVM 重用。可在 hadoop 的 mapred-site.xml 中配置。

## \#每日打卡#  5月31日面试题— Hive的存储格式了解哪些

Hive 是一种基于 Hadoop 分布式文件系统的数据仓库工具，它本身没有专门的数据存储格式，但是可以支持多种数据格式。在创建 Hive 表时，只需要指定数据的列分隔符和行分隔符，Hive 就可以解析数据。Hive 中包含以下几种数据模型：Table、ExternalTable、Partition 和 Bucket。其中，Table 和 ExternalTable 是 Hive 中的两种表类型，Table 的数据存储在 HDFS 中，而 ExternalTable 的数据存储位置可以在任意指定路径。Partition 和 Bucket 则是 Hive 表的数据分区和数据桶，可以用于对表数据进行分区和存储优化。
除了以上提到的数据模型，Hive 还支持多种数据格式，例如 Text、SequenceFile、ParquetFile、RCFILE 等。其中，Text 格式是一种简单的文本格式，SequenceFile 格式是一种二进制文件格式，ParquetFile 格式是一种列式存储格式，RCFILE 格式是一种面向行的二进制文件格式。这些格式都可以用于存储 Hive 表的数据。
总之，Hive 的存储格式主要是指用于存储 Hive 表数据的数据格式，这些格式可以是 Text、SequenceFile、ParquetFile、RCFILE 等，也可以是其他支持的数据格式。在创建 Hive 表时，需要指定数据的列分隔符和行分隔符，以便 Hive 可以正确解析数据。同时，Hive 还支持多种数据模型，如 Table、ExternalTable、Partition 和 Bucket，这些模型可以用于对表数据进行分区和存储优化。



## \#每日打卡# 6月1日面试题— 为什么要对数据仓库分层？

数据仓库分层的主要目的是提高数据处理的效率和灵活性，以满足不同业务需求。以下是一些具体原因：
1. 数据量庞大：随着企业业务的增长，数据量不断增加，导致数据处理和查询的时间越来越长。通过将数据分层，可以将数据划分为较小的范围，从而提高查询效率。
2. 提高数据质量：数据仓库中的数据可能来自不同的来源，数据质量可能不一致。通过分层，可以在不同层次上进行数据清洗和转换，以确保数据的质量和准确性。
3. 灵活性：随着业务需求的变化，需要对数据进行不同的处理和查询。通过将数据分层，可以更灵活地组合和处理数据，以满足不同的业务需求。
4. 可扩展性：随着业务增长，需要增加更多的数据存储和处理能力。通过将数据分层，可以更容易地扩展数据仓库的容量和处理能力。
5. 数据安全：数据仓库中的数据可能包含敏感信息，需要进行安全保护。通过将数据分层，可以将敏感数据和不敏感数据分开存储，以提高数据安全性。
总之，数据仓库分层可以提高数据处理的效率和灵活性，以满足不同业务需求，同时保护数据安全性。



## \#每日打卡# 6月2日面试题— 数仓建模常见的模型有哪些

数据仓库建模是为了将数据组织成适合分析和查询的格式。常见的数据仓库建模方法包括：
1. 维度建模：维度建模是一种将数据以维度和度量为核心进行建模的方法。维度是数据的分类，度量是数据的指标。这种方法被广泛应用于企业数据仓库中，特别是那些需要对业务数据进行深入分析的公司。
2. 范式建模：范式建模是一种基于关系型数据库的建模方法，它将数据分解为多个实体和关系，以确保数据的一致性、完整性和可靠性。这种方法适用于需要对数据进行严格管理和控制的场景。
3. DataVault 建模：DataVault 建模是一种将数据分为多个数据域进行建模的方法，每个数据域都有自己的数据模型和元数据。这种方法被广泛应用于金融和保险行业中，特别是那些需要对数据进行高度安全和保密的公司。
4. Anchor 建模：Anchor 建模是一种将数据以锚点为核心进行建模的方法，锚点是数据的关键点，用于连接不同的数据源和数据存储。这种方法适用于需要对多个数据源进行整合和统一管理的场景。
以上是常见的数据仓库建模方法，每种方法都有其适用的场景和优缺点，需要根据具体情况进行选择。



## \#每日打卡# 6月5日面试题— 数仓建模方法有哪些

数仓建模是指将业务数据转化为适合数据仓库存储和分析的数据模型的过程。常见的数仓建模方法包括：
1. 业务建模：从业务角度出发，将业务过程和数据流进行分解和细化，形成业务模型。
2. 概念建模：在业务模型的基础上，将业务概念抽象为数据概念，形成概念模型。
3. 逻辑建模：将概念模型中的概念实体和实体之间的关系进行数据库层次的逻辑化，形成逻辑模型。常见的逻辑建模方法包括雪花型、星型等。
4. 物理建模：将逻辑模型落实到具体的数据库产品中，考虑性能、存储和查询等方面的技术问题，形成物理模型。
5. 指标体系建设：在数仓建模过程中，建立适合业务需求的指标体系，用于监控和评估业务的状态和发展趋势。
6. 数据清洗和预处理：对原始数据进行清洗和预处理，保证数据的质量和准确性。
7. 数据加载和转换：将清洗和预处理后的数据加载到数据仓库中，并进行转换和整合，形成适合分析的数据模型。
8. 数据存储和管理：对数据仓库中的数据进行存储和管理，保证数据的安全性、完整性和可靠性。
以上是数仓建模的常见方法，不同的业务场景和数据类型可能会需要不同的建模方法。

## \#每日打卡# 6月6日面试题— 介绍下维度建模中的事实表和维度表

在维度建模中，事实表和维度表是两个基本的构成要素。事实表是存储业务事实数据的表，而维度表则是存储事实数据所需要的维度信息的表。事实表和维度表之间的关系通过键关联建立，这使得事实表能够与维度表进行关联，从而实现对数据的查询和分析。
事实表通常存储的是业务过程中产生的数值型数据，如销售额、库存量等。事实表中的每一行代表一个具体的业务事实，例如一个销售记录。事实表的每一列都代表一个具体的业务事实属性，例如销售日期、销售金额等。由于事实表中存储的是数值型数据，因此可以进行数值计算，如求和、平均值等。
维度表则存储了事实表所需要的维度信息，例如时间维度、地理维度、产品维度等。维度表中的每一行代表一个具体的维度值，例如一个具体的时间点、一个具体的产品名称等。维度表的每一列都代表一个具体的维度属性，例如时间维度中的年、季度、月、日等。通过维度表，可以对事实表中的数据进行切片、切块、过滤等操作，从而实现对数据的深入分析。
在维度建模中，事实表和维度表之间的关系是非常重要的，因为它们是查询和分析数据的基础。通常，事实表和维度表之间的关系是通过键关联建立的。例如，如果事实表中的销售记录包含销售日期、销售金额等属性，那么可以通过销售日期这个属性与维度表中的时间维度建立关联，从而实现对销售记录的按时间进行分析。

## \#每日打卡# 6月7日面试题 维度建模中事实表主要分为哪些种类

在维度建模中，事实表主要分为以下几种类型：
1. 事务型事实表：记录业务活动的事实，如订单支付金额、购买商品数量等。事务型事实表通常包含原子指标和修饰词，例如订单支付金额的原子指标是支付金额，修饰词可以是时间周期、订单状态等。
2. 存量型事实表：记录实体对象的历史统计信息，如商品总数、会员数量等。存量型事实表通常包含原子指标和维度属性，例如商品总数的原子指标是数量，维度属性可以是商品 ID、商品名称等。
3. 复合型事实表：在事务型事实表和存量型事实表的基础上复合而成，如渠道转化率、商品浏览量等。复合型事实表通常包含多个原子指标和修饰词，例如渠道转化率的原子指标是转化率，修饰词可以是时间周期、渠道名称等。
4. 统计型事实表：包含统计指标和维度属性，如自然月日均 UV、省份排名等。统计型事实表通常不创建原子指标，而是通过增加修饰词和维度属性来创建派生指标，例如人均 UV、省份排名等。
事实表的设计需要根据具体业务场景进行选择和调整，不同的事实表类型适用于不同的业务需求。同时，事实表和维度表的设计需要遵循一致性和可扩展性的原则，以便更好地支持数据分析和决策。

## \#每日打卡# 6月8日面试题— 数仓中指标与标签的区别

在数据仓库中，指标和标签是用来描述和分类数据的两种概念。它们有以下区别：
1. 概念不同：指标是用来定义、评价和描述特定事物的一种标准或方式，通常由名称和取值两部分构成。标签是人为设定的、根据业务场景需求，对目标对象运用一定的算法得到的高度精炼的特征标识，通常由几个字或符号组成。
2. 构成不同：指标通常由名称和取值构成，名称是对事物质与量两方面特点的命名，取值是指标在具体时间、地域、条件下的数量表现。标签通常就是简单的几个字或符号，是由一定的算法得到的高度精炼的特征标识。
3. 属性与特征不同：指标注重对事物及事件的过程进行全面的、体系化的描述，指标的描述范围更广泛，既包括过程也涵盖结果。标签比指标更有深度、更凝练，是对指标深度加工的结果，注重人物和实体对象的描述，侧重对局部特征和结果的描述，注重与具体业务场景的结合。
4. 价值评价方式不同：对指标的价值通常用好不好用”、“全不全面”来评价，对标签的价值通常用准不准”、“像不像”来评价。指标更注重与业务的结合，逻辑上更严谨，表现风格也比较严肃刻板。标签更侧重生活化、口语化和符号化，注重人物和实体对象的描述，描述的范围相对较窄。

## \#每日打卡# 6月9日面试题— 数仓中自然键、持久键、代理键的含义

在数据仓库中，自然键、持久键和代理键是三种不同的键，用于标识和连接表中的数据。
自然键（Natural Key）是表中自带的唯一标识符，通常是该表的主键。自然键是表中最自然、最本质的属性，可以用来唯一地标识一条记录，通常是数值类型或者字符串类型。
持久键（Persistent Key）是数据仓库中用于连接表的键，通常是外键。持久键是通过在表之间建立关系来实现数据连接的，可以让数据在表之间进行传递和连接。持久键通常是数值类型。
代理键（Surrogate Key）是数据仓库中用于连接表的键，通常是主键。代理键是人工创建的唯一标识符，用于代替自然键或者持久键，以便更好地连接表。代理键通常是整数类型，并且是自增的。
数据仓库中使用自然键、持久键和代理键的目的是为了建立数据之间的联系，以便更好地进行数据分析和查询。在使用这些键时，需要根据实际情况来选择最合适的键类型，以便更好地满足数据连接和查询的需求。

## \#每日打卡# 6月12日面试题— 遇到过数据漂移问题吗？数据漂移如何解决?

数据漂移是指在数据采集过程中，由于各种因素导致数据的误差或偏差逐渐累积，使得数据与实际情况不再相符。遇到数据漂移时，可以采取以下几种解决方法：
1. 检查数据采集设备：首先检查数据采集设备的工作状态是否正常，如传感器、测量仪器等是否准确、灵敏，是否受到外界干扰等因素。
2. 校准数据采集设备：对数据采集设备进行定期校准，以确保其准确性和精度。校准可以采用标准品、参考物质等进行比较，或者使用设备厂商提供的校准工具进行自检。
3. 改进数据采集方法：改进数据采集方法，如调整采样频率、增加采样点数、改进采样方式等，以提高数据采集的准确性和精度。
4. 采用数据漂移补偿技术：采用数据漂移补偿技术，通过对数据进行实时监测和分析，及时发现和消除数据漂移，以保证数据的准确性和精度。
5. 严格控制实验条件：在数据采集过程中，严格控制实验条件，如温度、湿度、压力等，以减少对数据采集的影响。
总之，遇到数据漂移时，需要根据实际情况采取相应的解决方法，以确保数据的准确性和精度。

## \#每日打卡# 6月13日面试题— 谈谈对元数据的理解？

元数据是关于数据的数据，是指附加在数据上的信息，用来描述数据的特征、来源、质量和其他属性。在数字图像领域，元数据是指存储在图像文件中的关于图像的信息，例如图像的尺寸、分辨率、颜色深度、拍摄日期、拍摄地点、拍摄者姓名等等。元数据是图像文件中必不可少的一部分，它可以帮助用户更好地管理和使用图像资源，同时也是图像处理的重要依据。
常见的元数据包括：图像描述、来源、作者、生产者、型号、方向、分辨率、分辨率单位、软件、固件版本、日期和时间、色相定位、Exif 信息位置、曝光时间、光圈系数、曝光程序、感光度、Exif 版本、创建时间、数字化时间、图像构造、压缩时每像素色彩位、曝光补偿、最大光圈、测光方式、光源、闪光灯、焦距、作者标记、FlashPix 版本、色域、图像宽度、图像高度、通用性扩展项定义、源文件、压缩比等等。
元数据的理解和使用对于数字图像处理和应用具有重要意义。通过了解元数据，用户可以更好地理解和使用图像资源，同时可以为图像处理和分析提供重要的依据。



## \#每日打卡# 6月14日面试题— 数仓中如何确定主题域的？

在数据仓库中，主题域是指按照数据分析应用的角度进行划分的，通常是联系较为紧密的数据主题的集合。主题域的划分是自上而下的，以业务分析视角来划分数据。一般是在完成业务需求调研之后，根据业务需求的特点，将数据从业务系统划分的数据域重新划分至不同的主题域。
举个例子，假设一个电商业务，其数据域可以划分为商品域、日志域、交易域、用户域、服务域等。而根据业务需求的特点，可以将这些数据域重新划分至不同的主题域，例如商品主题域、用户主题域、交易主题域等。在确定主题域时，需要考虑数据的完整性、一致性和可用性，确保数据能够在不同的主题域之间共享和重复使用。
确定主题域的关键是要从业务分析的角度出发，根据业务需求的特点进行划分，使得不同的主题域能够涵盖当前所有的业务需求，同时又能够将一个新业务无影响地被包含进来或者扩展出一个新的主题域。这样，才能够保证数据仓库的良好、规范的组织结构，能够对集合中的数据进行有序、有结构地分类组织和存储，从而支持管理决策。



## \#每日打卡# 6月15日面试题— 全量表、快照表、增量表、拉链表的区别和使用场景

全量表、增量表、快照表、拉链表是数据仓库中常用的四种表类型，它们有各自的特点和适用场景。
1. 全量表：全量表是最基本的表类型，它包含了所有的数据记录，无论这些记录是否发生变化。全量表通常只有一个分区或者没有分区，每次往全量表里面写数据都会覆盖之前的数据，不能记录数据的历史变化，只能截止到当前最新、全量的数据。全量表适用于需要获取所有数据的场景，例如数据分析和报表生成。
2. 增量表：增量表是基于全量表构建的，它只记录发生变化的数据。增量表每天新增的数据和改变的数据都会存储在当日的分区中；增量表记录每次增加的量，只报变化量，无变化的不用报。增量表设计过程，通常以相邻两天的数据为例，找出第二天未修改的数据，写入当前分区，将第二天新增和变化的数据写入当前分区，然后删除过期的分区。增量表适用于需要获取发生变化的数据场景，例如实时计算和流式处理。
3. 快照表：快照表是全量表的某个时间点的副本，它记录截止数据日期的全量数据。快照表可以在数据量大的情况下减少数据冗余和浪费存储空间，但每个分区存储的都是全量数据。快照表适用于需要获取某个时间点的数据的场景，例如数据备份和恢复。
4. 拉链表：拉链表是一种特殊的快照表，它能够解决快照表数据冗余问题，还能维护数据历史状态和最新状态，记录截止数据日期的全量数据，一个事物从开始，一直到当前状态的所有变化信息。拉链表通常适用于需要获取历史状态和最新状态的数据场景，例如数据挖掘和智能分析。
综上所述，全量表、增量表、快照表、拉链表各有特点，需要根据具体的业务场景选择合适的表类型。

## \#每日打卡# 6月16日面试题— 数仓建设中你认为最重要的是什么

在数仓建设中，最重要的是确保数据的质量和可靠性。数据质量是指数据的准确性、完整性、一致性、及时性和可靠性等方面的特征。数据质量是数仓建设的基础，只有保证数据质量，才能保证数仓的可靠性和稳定性。
为了确保数据的质量，需要建立严格的数据管理规范和流程，包括数据采集、数据清洗、数据转换、数据存储和数据备份等环节。同时，还需要建立数据质量监控和评估机制，及时发现和解决数据质量问题。
此外，数仓建设还需要考虑数据的安全性和隐私性，确保数据不会被非法获取和滥用。因此，在数仓建设中，需要采取有效的安全措施，包括访问控制、数据加密、数据备份和恢复等措施，以保障数据的安全性和隐私性。



## \#每日打卡# 6月19日面试题— Flink的运行必须依赖Hadoop组件吗

不一定。虽然 Flink 最初设计为与 Hadoop 生态系统一起使用，但自 Flink 1.9 版本开始，它已经可以独立于 Hadoop 运行。这意味着 Flink 可以部署在本地计算机、容器化环境（如 Docker、Kubernetes 等）或其他分布式环境中，而无需依赖 Hadoop 组件。 
当 Flink 与 Hadoop 一起使用时，它可以使用 Hadoop 分布式文件系统（HDFS）作为其持久化存储层，并利用 Hadoop YARN 资源调度器来管理集群中的容器。但是，这并非必需的，Flink 也可以使用其他分布式文件系统和资源调度器。例如，Flink 可以与 Apache Cassandra、Apache Hbase、Amazon S3 等存储系统集成，并使用 Kubernetes、Docker 等容器化技术来管理集群中的容器。
因此，Flink 的运行不一定依赖 Hadoop 组件，它可以在各种环境中独立运行。



## \#每日打卡# 6月20日面试题— Flink相比Spark Streaming有什么区别

Flink 和 Spark Streaming 都是流行的大数据流处理框架，它们有一些共同点，例如都能够处理实时数据流和批处理数据，但也存在一些不同之处。以下是 Flink 相比 Spark Streaming 的一些区别：
1. 运行模型：Flink 是基于事件驱动的流处理框架，它会从一个或多个流中注入事件，通过触发计算更新状态。而 Spark Streaming 则是微批处理，将数据流分成一个个微小批次进行处理。
2. 组件：Flink 的组件包括 JobManager 和 TaskManager，其中 JobManager 负责协调分布式执行，调度任务、协调 checkpoints、协调故障恢复等。而 Spark Streaming 的组件包括 Master、Worker、Driver 和 Executor，其中 Master 主要负责整体集群资源的管理和应用程序调度，Driver 用户入口程序执行的地方，即 SparkContext 执行的地方，主要是 DGA 生成、stage 划分、task 生成及调度，Executor 负责执行 task，反馈执行状态和执行结果。
3. 任务调度原理：Flink 的所有算子都是 lazy 形式的，调用 env.execute 会构建 jobgraph，client 端负责 Jobgraph 生成并提交它到集群运行。而 Spark Streaming 的操作算子分 action 和 transform，其中仅有 transform 是 lazy 形式，而且 DGA 生成、stage 划分、任务调度是在 driver 端进行的，在 client 模式下 driver 运行于客户端处。
4. 代码编写流程：Flink 调用 env.execute 相比于 Spark Streaming 少了设置批处理时间，而且 Flink 的所有算子都是 lazy 形式的。而 Spark Streaming 则需要设置批处理时间，并且操作算子分 action 和 transform，其中仅有 transform 是 lazy 形式。
总的来说，Flink 和 Spark Streaming 在流处理方面都有各自的优势和适用场景，选择哪个框架要根据具体的业务需求和技术栈来进行决策。

## \#每日打卡# 6月21日面试题 介绍下Flink的容错机制（checkpoint）

Flink 的容错机制（checkpoint）是一种用于处理数据流计算中的错误和故障的机制。在数据流计算中，由于数据量的庞大和计算的复杂性，可能会出现各种错误和故障，例如数据丢失、节点崩溃、网络中断等。而 Flink 的容错机制可以在这些错误和故障发生时，保证数据流计算的可靠性和正确性。
Flink 的容错机制基于 checkpoint 进行实现。checkpoint 是一个存储计算状态和元数据的持久化机制，它可以在特定时间点或者触发条件时，将计算状态和元数据保存到外部存储系统中。当发生错误或故障时，Flink 可以通过恢复最近的 checkpoint 来恢复计算状态，从而避免数据的丢失和计算的错误。
Flink 的 checkpoint 机制具有以下特点：

1. 一致性：确保所有算子在 checkpoint 时的状态是一致的，即保证了数据流计算的正确性。
2. 可靠性：checkpoint 可以配置为定期或基于事件触发，保证数据流计算的可靠性和安全性。
3. 灵活性：Flink 提供了多种状态后端（StateBackends）来支持不同的状态存储方式，例如内存、磁盘、网络等。
4. 高效性：Flink 的 checkpoint 机制采用了高效的算法和优化，保证了 checkpoint 的快速性和轻量性。
总的来说，Flink 的容错机制（checkpoint）为数据流计算提供了可靠和高效的容错和恢复机制，从而保证了数据流计算的可靠性和正确性。

Checkpoint 的实现原理主要包括以下几个方面：

1. 状态后端（StateBackends）：Flink 支持多种状态存储方式，包括内存状态后端、文件状态后端、数据库状态后端等。开发者可以根据自己的需求选择合适的状态后端。
2. 检查点（checkpoint）：检查点是 Flink 容错机制的核心概念。它是一个中间状态的快照，记录了此时数据流的计算状态和进度。检查点可以手动触发，也可以通过配置触发条件自动触发。
3. 检查点属性：检查点属性包括检查点的触发条件、触发时间间隔、存储位置等。开发者可以根据自己的需求配置检查点属性。
4. 从检查点恢复状态：当程序出现错误或系统崩溃时，可以从最近的检查点开始恢复状态，避免数据的丢失和重复计算。
5. 检查点算法：Flink 提供了多种检查点算法，包括 Exactly-Once、At-Least-Once、At-Most-Once 等。开发者可以根据自己的需求选择合适的检查点算法。
6. 保存点（savepoint）：保存点是 Flink 的另一种容错机制。它可以在数据流处理过程中，将当前的状态保存到外部存储系统中，以便在程序出现错误或系统崩溃时，可以重新从外部存储系统中恢复状态。
总之，Flink 的容错机制（checkpoint）是一种强大的工具，可以帮助开发者处理数据流计算中的错误和故障。通过定期保存中间状态，并提供可靠的恢复机制，Flink 可以保证数据处理的正确性和可靠性。

## \#每日打卡# 6月25日面试题 Flink是如何保证Exactly-once语义的

Flink 是一种流式大数据处理引擎，它可以处理实时数据流并保证数据处理的准确性。为了保证数据处理的准确性，Flink 采用了基于状态的 CheckPoint 机制和 Exactly-Once 语义。
在 Flink 中，数据处理是有状态的，每个节点在执行计算任务时会将相关状态信息保存到状态后端 (State-Backend)，可配置成内存存，文件系统或外部数据库。Flink 中的 CheckPoint 是一个触发器，它可以定期或基于事件触发，用于记录当前状态以便在应用程序失败时进行恢复。
Flink 中的 Exactly-Once 语义是指，对于每个事件，只会被处理一次，确保事件被正确地处理和消费。为了实现 Exactly-Once 语义，Flink 采用了 Chandy-Lamport 算法来保证全局状态一致性。在 Flink 中，Barrier 是一个用于区分每次 CheckPoint 界限的概念，它类似于一个隔板，将属于不同批次的 CheckPoint 数据进行隔开，从而保证每一次的 CheckPoint 过程所有节点状态保存的全局一致性。
具体来说，当一个 Barrier 从 Source 节点开始生成，然后依次向下游传递，下游节点一旦接收到 Barrier，就暂停处理数据，开始进行当前状态的保存。然后接着处理后面的数据。Flink 中的 CheckPoint 机制背后的实现算法是 Chandy-Lamport 算法，它可以应用于分布式系统上在没有全局统一时钟或全局统一时钟不可靠的情况下保证全局状态一致性。
综上所述，Flink 通过基于状态的 CheckPoint 机制和 Exactly-Once 语义来保证数据处理的准确性。Flink 采用 Chandy-Lamport 算法来保证全局状态一致性，同时通过 Barrier 来区分每次 CheckPoint 的界限，从而实现 Exactly-Once 语义。



## \#每日打卡# 6月26日面试题— Flink 中的 Time 有哪几种

在 Flink 中，Time 是一个重要的概念，它用于描述事件发生的时间以及处理事件的时间。Flink 中支持三种时间类型：
1. ProcessingTime：处理时间，即当前机器处理该事件的时间，也就是进入某个算子时的系统时间。ProcessingTime 具有最好的性能和最低的延迟，但它不能解决数据乱序的问题。
2. IngestionTime：摄入时间，即数据进入 Flink 框架的时间。在 SourceOperator 中设置，每个事件拿到当前时间作为时间戳，后续的时间窗口基于该时间。相比 ProcessingTime，IngestionTime 可以提供更可预测的结果，但它也不能解决数据乱序的问题。
3. EventTime：事件时间，即每条事件在它产生的时候记录的时间。该时间记录在事件中，在处理的时候可以被提取出来。EventTime 依赖于事件本身，而跟物理时钟没有关系。利用 EventTime 编程可以解决乱序、延时、或者数据重放等问题，但 EventTime 存在一定的延时，因此自然的需要延时和无序事件等待一段时间。
在 Flink 程序中，通常需要设置时间特性来定义数据用什么时间，在时间窗口处理中使用什么时间。例如，可以使用 ingestionTimeCharacteristic 来设置使用摄入时间，或使用 processingTimeCharacteristic 来设置使用处理时间。同时，也可以使用 eventTimeCharacteristic 来设置使用事件时间。在设置时间特性时，需要考虑到数据源的特性以及业务需求，选择最合适的时间类型来处理数据。

## \#每日打卡# 6月27日面试题— Flink中对于迟到数据是怎么处理的

在 Flink 中，迟到数据是指系统的事件时间戳在经过延迟元素时间戳之后的时间到达的数据，它没有被 watermark 和 Window 机制处理，因为是在窗口关闭后才到达的数据。Flink 默认将迟到数据直接丢弃。
但是，在某些场景下，我们可能需要处理迟到数据。Flink 提供了一些机制来解决延迟数据的问题：
1. WaterMark: 水印机制是 Flink 中处理延迟数据的一种方式。通过给每个算子添加一个水印，可以将延迟数据标注出来，然后在后续算子中进行处理。可以使用 Flink 提供的 `add_source` 和 `add_sink` 方法来添加水印。
2. AllowedLateness: 允许延迟性是 Flink 中处理延迟数据的另一种方式。它允许数据在到达算子时延迟一段时间，而不被视为迟到数据。可以使用 Flink 提供的 `set_timeout` 方法来设置允许的延迟时间。
3. SideOutput: 侧输出是 Flink 中处理延迟数据的另一种方式。使用侧输出，可以将延迟数据输出到一个独立的聚合算子中，然后在后续算子中进行处理。可以使用 Flink 提供的 `add_source` 和 `add_sink` 方法来添加侧输出。
在处理延迟数据时，需要根据具体的业务场景选择适合的处理方式。使用水印机制或允许延迟性可能会导致数据的准确性受到影响，而使用侧输出则可能会增加代码的复杂度和资源的消耗。

## \#每日打卡# 6月28日面试题— Flink中的状态存储

在 Flink 中，状态是指算子在处理数据流时所需要保持的信息。这些信息可以是某个时刻的数据，也可以是计算结果的中间状态。

Flink 中的状态可以分为两种类型：有状态和无状态。

- 有状态算子需要维护一个状态变量，用于存储计算所需的数据。这些状态变量可以是键控状态值、列表状态、映射状态等不同类型。
- 无状态算子则不需要维护状态变量，其计算结果只与输入数据有关。

Flink 中的状态存储有两种方式：内存和外部存储。

- 内存存储是指将状态变量存储在算子内部的内存中，这种方式适用于状态变量较小的情况。

- 外部存储则是将状态变量存储在外部存储系统中，如 HDFS、HBase 等。当状态变量过大时，内存存储会变得不可行，此时需要使用外部存储。

Flink 还提供了一种称为 Checkpoint 的机制，用于定期将应用程序状态保存到远程且持久的存储系统中。

Checkpoint 是 Flink 中保证状态可靠性的重要手段，它可以在应用程序或计算机故障时恢复状态。Flink 还支持将状态存储在分布式文件系统中，如 HDFS，以保证状态的可靠性和可扩展性。
总之，Flink 中的状态存储是指算子在处理数据流时所需要保持的信息，这些信息可以分为有状态和无状态两种类型。Flink 提供了多种状态存储方式，包括内存存储和外部存储，以满足不同情况下的需求。而 Checkpoint 机制则是 Flink 中保证状态可靠性的重要手段。



## \#每日打卡# 6月29日面试题— Flink是如何支持流批一体的

Flink 是一款专注于无限流处理的分布式计算框架，但它也可以支持批处理。Flink 通过一个底层引擎同时支持流处理和批处理，这个引擎叫做 Flink Execution Engine。在 Flink 中，流处理和批处理的核心概念是数据流和数据集。数据流是连续的、无限制的数据序列，而数据集是离散的、有限量的数据集合。
Flink 支持批处理的方式是通过 DataSet API。DataSet API 提供了一种以数据集为单位的编程模型，可以方便地对有限数据集进行处理。在 Flink 中，可以使用 DataStream API 将数据流转换为数据集，然后使用 DataSet API 对数据集进行处理。例如，可以使用以下代码实现对某个网站访问者计数的批处理：
val countVisits = env.fromElements(visits).groupBy(region).sum(visits)  
在这个例子中，fromElements方法将数据流转换为数据集，groupBy方法将数据集分组按照地区，sum方法对每个地区的访问次数进行求和。这种批处理方式的优点是可以使用 Flink 强大的 DataStream API 进行数据处理，同时也可以支持批量数据的处理。
Flink 还提供了一些机制来支持流批一体，包括检查点机制和状态机制、水印机制、窗口和触发器等。这些机制可以用于实现容错、有状态的处理、事件时钟和数据处理的限制等功能。例如，可以使用水印机制来实现事件时钟，窗口和触发器可以用于限制数据处理的范围和时间。这些机制可以帮助 Flink 在处理无限数据流的同时，也可以支持批量数据的处理。

## \#每日打卡# 6月30日面试题— Flink的内存管理是如何做的

Flink 是基于 JVM 的大数据处理引擎，针对大数据流式处理的特点，Flink 定制了自己的内存管理策略。Flink 将内存分为堆内内存和堆外内存，按照用途可以划分为 task 所用内存，networkmemory、managedmemory、以及 framework 所用内存，其中 tasknetworkmanaged 所用内存计入 slot 内存。framework 为 taskmanager 公用。
堆内内存包含用户代码所用内存、heapstatebackend、框架执行所用内存。堆外内存是未经 JVM 虚拟化的内存，直接映射到操作系统的内存地址，堆外内存包含框架执行所用内存，JVM 堆外内存、Direct、native 等。
Flink 使用内存划分为堆内内存和堆外内存，可以避免 Java 对象存储密度低、fullgc 时消耗性能、gc 存在 stw 的问题，同时针对频繁序列化和反序列化问题，Flink 使用堆内堆外内存可以直接在一些场景下操作二进制数据，减少序列化反序列化的消耗。
Flink 内存管理和 Spark 的 Tungsten 的内存管理的出发点很相似，都可以使用堆内和堆外内存，并且使用本地内存可以减少序列化和反序列化的消耗。此外，Flink 还基于 CPU L1 L2 L3 高速缓存的机制以及局部性原理，设计使用缓存友好的数据结构，可以提高数据处理的效率。同时，Flink 的 ManagedMemory 主要用于 RocksDBStateBackend 和批处理算子，也属于 native memory 的范畴。
Flink 通过使用堆内和堆外内存、本地内存以及缓存友好的数据结构，提高了大数据处理的效率和稳定性，同时避免了 Java 对象存储密度低、fullgc 时消耗性能、gc 存在 stw 的问题，以及频繁序列化和反序列化问题。